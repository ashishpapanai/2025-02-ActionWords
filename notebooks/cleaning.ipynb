{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5159e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7cec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.io.stata.StataReader(\"../dta files/2012.dta\", convert_categoricals=True) as rdr:\n",
    "    cols = [c for c in rdr.variable_labels().keys() if c not in {\"ISCO88\", \"SPISCO88\"}]\n",
    "    df_2012 = rdr.read(columns=cols)\n",
    "\n",
    "with pd.io.stata.StataReader(\"../dta files/2002.dta\", convert_categoricals=True) as rdr:\n",
    "    cols = [c for c in rdr.variable_labels().keys() if c not in [\"v241\",\"v247\"]]\n",
    "    df_2002 = rdr.read(columns=cols)\n",
    "\n",
    "df_2022 = pd.read_stata(\"../dta files/2022.dta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8116037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002_clean = pd.read_csv(\"../data/final_csv/2002_final.csv\")\n",
    "df_2012_clean = pd.read_csv(\"../data/final_csv/2012_final.csv\")\n",
    "df_2022_clean = pd.read_csv(\"../data/final_csv/2022_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "edf2cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban_rural\n",
      "['Urban' 'Town' 'Rural' nan]\n",
      "spouse_work_status\n",
      "['Retired' 'Paid work' 'Domestic work' 'DK/No answer' 'Other' 'Unemployed'\n",
      " 'Education' 'Help family member' 'Sick/Disabled']\n",
      "sex\n",
      "['Female' 'Male' nan]\n",
      "code_higher_income\n",
      "['Respondent has higher income' nan 'Same income'\n",
      " 'Partner has higher income' 'Partner has no income'\n",
      " 'Respondent has no income']\n",
      "code_income_control\n",
      "[nan 'Financial control is shared'\n",
      " 'Financial control is with the respondent'\n",
      " 'Financial control is separate' 'Financial control is with the partner']\n",
      "hh_wrk_hrs\n",
      "[30.  5. 16. 28. 12. 10. 20. 26.  8. 35.  3. 23. 50. 14.  7. 18.  4. nan\n",
      "  6.  9. 15. 40.  2. 25. 21. 39. 60. 24. 11. 56. 37. 22. 70. 17. 45. 65.\n",
      " 38. 31. 27. 32.  0. 36. 42. 48. 49. 46. 52. 13. 55.  1. 29. 19. 90. 84.\n",
      " 80. 72. 34. 51. 54. 95. 85. 47. 58. 78. 33. 44. 75. 43. 82. 63. 86. 59.\n",
      " 64. 41. 68. 62. 74. 87. 77. 94. 67. 66. 57. 71. 88. 76. 91.]\n",
      "work_status\n",
      "['Paid work' 'Unemployed' 'Retired' 'Domestic work' 'Other' 'Education'\n",
      " 'DK/No answer' 'Sick/Disabled']\n",
      "marital\n",
      "['Married' nan 'Single' 'Divorced' 'Separated' 'Widowed']\n",
      "wrk_hrs\n",
      "[30. nan  8. 50. 40. 15. 27.  6. 16. 28. 46. 58. 38. 18. 20. 56. 45. 10.\n",
      " 55. 35. 32. 24. 23. 48. 36. 26. 60. 52. 42. 12. 22. 25. 14.  3. 73. 37.\n",
      " 33. 41. 21.  5. 39. 31. 70. 72. 65. 44. 76. 19. 64. 13. 75. 80. 54.  4.\n",
      " 43. 63.  7. 17. 96. 53. 47. 85. 34. 57. 90. 66. 68. 51. 87. 11. 82. 95.\n",
      " 49. 84. 59. 29.  9. 62. 89. 61. 86. 83.  1. 78.  2. 88. 69. 67. 77. 91.\n",
      " 71. 79. 94. 81. 74.]\n",
      "educ_4_label\n",
      "['Post-sec / Short tertiary' 'University+' 'Secondary' 'No/Primary' nan]\n",
      "age\n",
      "[46. 40. 68. 69. 44. 49. 48. 47. 67. 62. 58. 37. 39. 51. 55. 23. 52. 71.\n",
      " 54. 42. 32. 35. 59. 60. 50. 27. 56. 33. 64. 53. 34. 43. 38. nan 65. 31.\n",
      " 29. 57. 22. 74. 41. 25. 28. 73. 70. 63. 61. 45. 36. 19. 20. 30. 66. 24.\n",
      " 26. 18. 21. 81. 72. 75. 78. 76. 79. 80. 77. 82. 87. 85. 89. 86. 84. 83.\n",
      " 88. 91. 92. 90. 94. 96. 17. 16. 93. 15.]\n",
      "urban_rural\n",
      "['Town' 'Rural' 'Urban' nan 'Suburban']\n",
      "spouse_work_status\n",
      "['Paid work' 'NAP' 'Domestic work' 'DK/No answer' 'Retired'\n",
      " 'Sick/Disabled' 'Unemployed' 'Education' 'Other'\n",
      " 'Military/Community service' 'Apprentice/Trainee']\n",
      "sex\n",
      "['Male' 'Female' 'No answer']\n",
      "code_higher_income\n",
      "[nan 'Respondent has no income' 'Partner has no income'\n",
      " 'Respondent has higher income' 'Partner has higher income']\n",
      "code_income_control\n",
      "['Financial control is with the partner' 'Financial control is shared' nan\n",
      " 'Financial control is separate'\n",
      " 'Financial control is with the respondent']\n",
      "hh_wrk_hrs\n",
      "[ 7. 84. 24. 21. 20.  0. 56. 22. 40. 14. 30. 35. 28.  6. 42.  3. 10. 36.\n",
      " 38.  2.  4. 12. 50. 15. 70.  5. nan  1.  8. 18. 16. 11. 80. 48. 49. 75.\n",
      " 25. 32. 45. 60. 94.  9. 90. 29. 65. 54. 72. 34. 39. 95. 23. 26. 13. 37.\n",
      " 17. 31. 46. 77. 19. 58. 27. 67. 53. 55. 85. 63. 78. 52. 44. 68. 91. 43.\n",
      " 33. 87. 47. 41. 51. 64. 57. 71. 66. 62. 82. 59. 76. 86. 92. 73. 93. 89.\n",
      " 74.]\n",
      "work_status\n",
      "['Paid work' 'Unemployed' 'Education' 'Domestic work' 'Retired'\n",
      " 'Sick/Disabled' 'Other' 'DK/No answer' 'Apprentice/Trainee'\n",
      " 'Military/Community service']\n",
      "marital\n",
      "['Civil partnership' 'Single' 'Married' 'Divorced' 'Widowed' nan]\n",
      "wrk_hrs\n",
      "[60. 24. nan 36. 40. 48. 16. 72. 84. 30. 64. 45. 70. 50. 35. 25. 15. 12.\n",
      " 71. 44. 56. 96. 21. 20.  9.  4. 14.  6. 10. 52. 80. 55. 28. 63. 43. 75.\n",
      "  8. 89. 27. 90.  5. 42. 38. 54. 34. 66. 46. 78. 18. 85. 68. 51. 49. 76.\n",
      " 53. 65. 13.  7. 47. 22. 32. 19. 23. 31. 41. 37. 26. 29. 58. 33. 17.  3.\n",
      " 11. 86. 39. 59.  2. 74. 62. 94. 77. 91. 57. 81. 82. 67. 92. 88. 73.  1.\n",
      " 87. 61. 69. 95.]\n",
      "educ_4_label\n",
      "['Secondary' 'No/Primary' 'Post-sec / Short tertiary' 'University+' nan]\n",
      "age\n",
      "[ 35.  24.  18.  61.  55.  53.  22.  39.  49.  28.  29.  31.  45.  59.\n",
      "  54.  20.  64.  70.  76.  75.  32.  63.  62.  25.  26.  27.  21.  41.\n",
      "  50.  57.  46.  48.  36.  74.  23.  34.  43.  47.  51.  33.  71.  81.\n",
      "  52.  42.  82.  66.  38.  19.  40.  72.  60.  73.  69.  56.  37.  85.\n",
      "  65.  79.  77.  78.  68.  84.  67.  30.  58.  86.  91.  87.  44.  83.\n",
      "  80.  90.  92.  88.  nan  93.  89.  95.  94.  96.  17.  98. 102.  16.\n",
      "  15.  97. 101.]\n",
      "urban_rural\n",
      "['Suburban' 'Town' 'Rural' 'Urban' nan]\n",
      "spouse_work_status\n",
      "['NAP' 'Paid work' 'Retired' 'DK/No answer' 'Domestic work' 'Education'\n",
      " 'Sick/Disabled' 'Military/Community service' 'Other' 'Unemployed'\n",
      " 'Apprentice/Trainee']\n",
      "sex\n",
      "['Female' 'Male' nan]\n",
      "code_higher_income\n",
      "[nan]\n",
      "code_income_control\n",
      "[nan 'Financial control is shared'\n",
      " 'Financial control is with the respondent'\n",
      " 'Financial control is separate' 'Financial control is with the partner']\n",
      "hh_wrk_hrs\n",
      "[18.  8.  3.  5. 10. 28.  4. 20.  6. 17. 32. 15.  7. 13.  2. 42. 14. 49.\n",
      " 21. 12. 35. 60. 16.  9. 30.  1. 25.  0. 50. 34. 11. 40. 24. 26. 45. 22.\n",
      " nan 31. 71. 47. 19. 58. 95. 38. 39. 80. 85. 36. 70. 84. 90. 27. 48. 44.\n",
      " 29. 23. 56. 37. 63. 46. 33. 72. 68. 64. 55. 65. 82. 59. 88. 66. 78. 73.\n",
      " 62. 75. 54. 74. 52. 43. 76. 86. 67. 41. 53. 94. 91. 61. 83.]\n",
      "work_status\n",
      "['Paid work' 'Domestic work' 'Education' 'Unemployed' 'Retired'\n",
      " 'Sick/Disabled' 'DK/No answer' 'Other' 'Apprentice/Trainee'\n",
      " 'Military/Community service']\n",
      "marital\n",
      "['Divorced' 'Single' 'Married' 'Civil partnership' nan 'Widowed']\n",
      "wrk_hrs\n",
      "[35. 40. 20. 50. nan 30. 60. 38. 15. 45. 10. 78. 48.  2. 25. 32. 80. 36.\n",
      " 16. 44. 70. 42. 39.  3. 27. 24. 46.  8. 28. 31. 22. 33. 12. 47. 55. 19.\n",
      " 26. 13. 41.  7. 37.  5. 18. 65.  6. 11. 17. 23. 43. 95. 14. 21. 72. 34.\n",
      " 52. 58.  9. 29.  4. 54. 56. 90. 84. 63. 75. 57. 96. 51.  1. 64. 92. 71.\n",
      " 77. 81. 91. 83. 49. 86. 62. 66. 53. 68. 74. 76. 85. 59. 88. 67. 73. 82.\n",
      " 61. 93. 89.]\n",
      "educ_4_label\n",
      "[nan 'Post-sec / Short tertiary' 'Secondary' 'University+' 'No/Primary']\n",
      "age\n",
      "[ 46.  59.  28.  60.  43.  65.  24.  33.  72.  29.  42.  55.  66.  26.\n",
      "  76.  34.  63.  49.  27.  30.  54.  40.  74.  53.  73.  51.  35.  41.\n",
      "  19.  45.  61.  71.  64.  62.  36.  68.  57.  56.  70.  47.  23.  52.\n",
      "  69.  37.  77.  80.  20.  21.  39.  48.  38.  44.  22.  50.  79.  67.\n",
      "  31.  58.  75.  25.  83.  32.  88.  18.  92.  84.  17.  82.  78.  81.\n",
      "  89.  16.  86.  90.   9.  96.  93.  87.  91.  85.  94.  95.  98. 100.\n",
      "  97.  99.  15.]\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_2002_clean, df_2012_clean, df_2022_clean]\n",
    "cleaned_vars = [\n",
    "    \"urban_rural\",\n",
    "    \"spouse_work_status\",\n",
    "    \"sex\",\n",
    "    \"code_higher_income\",\n",
    "    \"code_income_control\",\n",
    "    \"hh_wrk_hrs\",\n",
    "    \"work_status\",\n",
    "    \"marital\",\n",
    "    \"wrk_hrs\",\n",
    "    \"educ_4_label\",\n",
    "    \"age\",\n",
    "]\n",
    "for df in dfs:\n",
    "    for var in cleaned_vars:\n",
    "        print(var)\n",
    "        print(df[var].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "14c4c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(series):\n",
    "    \"\"\"\n",
    "    Normalize a numeric series to 0-1 range using min-max scaling.\n",
    "    \"\"\"\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "# Apply to eg_score columns\n",
    "df_2002_clean['eg_score_norm'] = normalize_column(df_2002_clean['eg_score'])\n",
    "df_2012_clean['eg_score_norm'] = normalize_column(df_2012_clean['eg_score'])\n",
    "df_2022_clean['eg_score_norm'] = normalize_column(df_2022_clean['eg_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "215ab9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = pd.read_csv(\"../common_question_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "820ae90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban_rural\n",
      "spouse_work_status\n",
      "sex\n",
      "code_higher_income\n",
      "code_income_control\n",
      "hh_wrk_hrs\n",
      "work_status\n",
      "marital\n",
      "wrk_hrs\n",
      "educ_4_label\n",
      "age\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_mapping.iterrows():\n",
    "    if row[\"COMMON_VAR\"] in df_2022_clean.columns:\n",
    "        print(row[\"COMMON_VAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "64b121b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Variable names standardized:\n",
      "  - var_02 (2002): lowercase 'v'\n",
      "  - var_12 (2012): uppercase 'V'\n",
      "  - var_22 (2022): lowercase 'v'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for index, row in df_mapping.iterrows():\n",
    "    # Fix var_02: should start with lowercase \"v\"\n",
    "    if row[\"COMMON_VAR\"] not in df_2022_clean.columns:\n",
    "        if pd.notna(row[\"var_02\"]) and isinstance(row[\"var_02\"], str):\n",
    "            if re.match(r'^V\\d', row[\"var_02\"]):\n",
    "                df_mapping.at[index, \"var_02\"] = 'v' + row[\"var_02\"][1:]\n",
    "        \n",
    "        # Fix var_12: should start with uppercase \"V\"\n",
    "        if pd.notna(row[\"var_12\"]) and isinstance(row[\"var_12\"], str):\n",
    "            if re.match(r'^v\\d', row[\"var_12\"]):\n",
    "                df_mapping.at[index, \"var_12\"] = 'V' + row[\"var_12\"][1:]\n",
    "        \n",
    "        # Fix var_22: should start with lowercase \"v\"\n",
    "        if pd.notna(row[\"var_22\"]) and isinstance(row[\"var_22\"], str):\n",
    "            if re.match(r'^V\\d', row[\"var_22\"]):\n",
    "                df_mapping.at[index, \"var_22\"] = 'v' + row[\"var_22\"][1:]\n",
    "\n",
    "print(\"✓ Variable names standardized:\")\n",
    "print(\"  - var_02 (2002): lowercase 'v'\")\n",
    "print(\"  - var_12 (2012): uppercase 'V'\")\n",
    "print(\"  - var_22 (2022): lowercase 'v'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8b2f5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added 47 common variables to all clean dataframes\n",
      "  2002: (46638, 69)\n",
      "  2012: (61754, 69)\n",
      "  2022: (45762, 69)\n"
     ]
    }
   ],
   "source": [
    "# # Add common variables to clean dataframes from raw dataframes\n",
    "# # Ensure all COMMON_VAR columns exist in all dataframes for consistency\n",
    "# for index, row in df_mapping.iterrows():\n",
    "#     common_var = row[\"COMMON_VAR\"]\n",
    "#     if common_var in df_2002_clean.columns:\n",
    "#         print(common_var)\n",
    "#     if common_var not in df_2002_clean.columns:\n",
    "#         # 2002\n",
    "#         if pd.notna(row[\"var_02\"]) and row[\"var_02\"] in df_2002.columns:\n",
    "#             df_2002_clean[common_var] = df_2002[row[\"var_02\"]].values\n",
    "#         else:\n",
    "#             # Create column with NaN if variable doesn't exist for this year\n",
    "#             if common_var not in df_2002_clean.columns:\n",
    "#                 df_2002_clean[common_var] = np.nan\n",
    "        \n",
    "#         # 2012\n",
    "#         if pd.notna(row[\"var_12\"]) and row[\"var_12\"] in df_2012_clean.columns:\n",
    "#             df_2012_clean[common_var] = df_2012[row[\"var_12\"]].values\n",
    "#         else:\n",
    "#             # Create column with NaN if variable doesn't exist for this year\n",
    "#             if common_var not in df_2012_clean.columns:\n",
    "#                 df_2012_clean[common_var] = np.nan\n",
    "        \n",
    "#         # 2022\n",
    "#         if pd.notna(row[\"var_22\"]) and row[\"var_22\"] in df_2022.columns:\n",
    "#             df_2022_clean[common_var] = df_2022[row[\"var_22\"]].values\n",
    "#         else:\n",
    "#             # Create column with NaN if variable doesn't exist for this year\n",
    "#             if common_var not in df_2022_clean.columns:\n",
    "#                 df_2022_clean[common_var] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "for _, row in df_mapping.iterrows():\n",
    "    common_var = row[\"COMMON_VAR\"]\n",
    "\n",
    "    # ---- 2002 ----\n",
    "    if common_var not in df_2002_clean.columns:\n",
    "        v = row.get(\"var_02\", np.nan)\n",
    "        if pd.notna(v) and v in df_2002.columns:\n",
    "            df_2002_clean[common_var] = df_2002[v].reindex(df_2002_clean.index)\n",
    "        else:\n",
    "            df_2002_clean[common_var] = np.nan\n",
    "\n",
    "    # ---- 2012 ----\n",
    "    if common_var not in df_2012_clean.columns:\n",
    "        v = row.get(\"var_12\", np.nan)\n",
    "        if pd.notna(v) and v in df_2012.columns:\n",
    "            df_2012_clean[common_var] = df_2012[v].reindex(df_2012_clean.index)\n",
    "        else:\n",
    "            df_2012_clean[common_var] = np.nan\n",
    "\n",
    "    # ---- 2022 ----\n",
    "    if common_var not in df_2022_clean.columns:\n",
    "        v = row.get(\"var_22\", np.nan)\n",
    "        if pd.notna(v) and v in df_2022.columns:\n",
    "            df_2022_clean[common_var] = df_2022[v].reindex(df_2022_clean.index)\n",
    "        else:\n",
    "            df_2022_clean[common_var] = np.nan\n",
    "\n",
    "\n",
    "print(f\"✓ Added {len(df_mapping)} common variables to all clean dataframes\")\n",
    "print(f\"  2002: {df_2002_clean.shape}\")\n",
    "print(f\"  2012: {df_2012_clean.shape}\")\n",
    "print(f\"  2022: {df_2022_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "b44accda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban_rural\n",
      "['Urban' 'Town' 'Rural' nan]\n",
      "spouse_work_status\n",
      "['Retired' 'Paid work' 'Domestic work' 'DK/No answer' 'Other' 'Unemployed'\n",
      " 'Education' 'Help family member' 'Sick/Disabled']\n",
      "sex\n",
      "['Female' 'Male' nan]\n",
      "code_higher_income\n",
      "['Respondent has higher income' nan 'Same income'\n",
      " 'Partner has higher income' 'Partner has no income'\n",
      " 'Respondent has no income']\n",
      "code_income_control\n",
      "[nan 'Financial control is shared'\n",
      " 'Financial control is with the respondent'\n",
      " 'Financial control is separate' 'Financial control is with the partner']\n",
      "hh_wrk_hrs\n",
      "[30.  5. 16. 28. 12. 10. 20. 26.  8. 35.  3. 23. 50. 14.  7. 18.  4. nan\n",
      "  6.  9. 15. 40.  2. 25. 21. 39. 60. 24. 11. 56. 37. 22. 70. 17. 45. 65.\n",
      " 38. 31. 27. 32.  0. 36. 42. 48. 49. 46. 52. 13. 55.  1. 29. 19. 90. 84.\n",
      " 80. 72. 34. 51. 54. 95. 85. 47. 58. 78. 33. 44. 75. 43. 82. 63. 86. 59.\n",
      " 64. 41. 68. 62. 74. 87. 77. 94. 67. 66. 57. 71. 88. 76. 91.]\n",
      "work_status\n",
      "['Paid work' 'Unemployed' 'Retired' 'Domestic work' 'Other' 'Education'\n",
      " 'DK/No answer' 'Sick/Disabled']\n",
      "marital\n",
      "['Married' nan 'Single' 'Divorced' 'Separated' 'Widowed']\n",
      "wrk_hrs\n",
      "[30. nan  8. 50. 40. 15. 27.  6. 16. 28. 46. 58. 38. 18. 20. 56. 45. 10.\n",
      " 55. 35. 32. 24. 23. 48. 36. 26. 60. 52. 42. 12. 22. 25. 14.  3. 73. 37.\n",
      " 33. 41. 21.  5. 39. 31. 70. 72. 65. 44. 76. 19. 64. 13. 75. 80. 54.  4.\n",
      " 43. 63.  7. 17. 96. 53. 47. 85. 34. 57. 90. 66. 68. 51. 87. 11. 82. 95.\n",
      " 49. 84. 59. 29.  9. 62. 89. 61. 86. 83.  1. 78.  2. 88. 69. 67. 77. 91.\n",
      " 71. 79. 94. 81. 74.]\n",
      "educ_4_label\n",
      "['Post-sec / Short tertiary' 'University+' 'Secondary' 'No/Primary' nan]\n",
      "age\n",
      "[46. 40. 68. 69. 44. 49. 48. 47. 67. 62. 58. 37. 39. 51. 55. 23. 52. 71.\n",
      " 54. 42. 32. 35. 59. 60. 50. 27. 56. 33. 64. 53. 34. 43. 38. nan 65. 31.\n",
      " 29. 57. 22. 74. 41. 25. 28. 73. 70. 63. 61. 45. 36. 19. 20. 30. 66. 24.\n",
      " 26. 18. 21. 81. 72. 75. 78. 76. 79. 80. 77. 82. 87. 85. 89. 86. 84. 83.\n",
      " 88. 91. 92. 90. 94. 96. 17. 16. 93. 15.]\n",
      "urban_rural\n",
      "['Town' 'Rural' 'Urban' nan 'Suburban']\n",
      "spouse_work_status\n",
      "['Paid work' 'NAP' 'Domestic work' 'DK/No answer' 'Retired'\n",
      " 'Sick/Disabled' 'Unemployed' 'Education' 'Other'\n",
      " 'Military/Community service' 'Apprentice/Trainee']\n",
      "sex\n",
      "['Male' 'Female' 'No answer']\n",
      "code_higher_income\n",
      "[nan 'Respondent has no income' 'Partner has no income'\n",
      " 'Respondent has higher income' 'Partner has higher income']\n",
      "code_income_control\n",
      "['Financial control is with the partner' 'Financial control is shared' nan\n",
      " 'Financial control is separate'\n",
      " 'Financial control is with the respondent']\n",
      "hh_wrk_hrs\n",
      "[ 7. 84. 24. 21. 20.  0. 56. 22. 40. 14. 30. 35. 28.  6. 42.  3. 10. 36.\n",
      " 38.  2.  4. 12. 50. 15. 70.  5. nan  1.  8. 18. 16. 11. 80. 48. 49. 75.\n",
      " 25. 32. 45. 60. 94.  9. 90. 29. 65. 54. 72. 34. 39. 95. 23. 26. 13. 37.\n",
      " 17. 31. 46. 77. 19. 58. 27. 67. 53. 55. 85. 63. 78. 52. 44. 68. 91. 43.\n",
      " 33. 87. 47. 41. 51. 64. 57. 71. 66. 62. 82. 59. 76. 86. 92. 73. 93. 89.\n",
      " 74.]\n",
      "work_status\n",
      "['Paid work' 'Unemployed' 'Education' 'Domestic work' 'Retired'\n",
      " 'Sick/Disabled' 'Other' 'DK/No answer' 'Apprentice/Trainee'\n",
      " 'Military/Community service']\n",
      "marital\n",
      "['Civil partnership' 'Single' 'Married' 'Divorced' 'Widowed' nan]\n",
      "wrk_hrs\n",
      "[60. 24. nan 36. 40. 48. 16. 72. 84. 30. 64. 45. 70. 50. 35. 25. 15. 12.\n",
      " 71. 44. 56. 96. 21. 20.  9.  4. 14.  6. 10. 52. 80. 55. 28. 63. 43. 75.\n",
      "  8. 89. 27. 90.  5. 42. 38. 54. 34. 66. 46. 78. 18. 85. 68. 51. 49. 76.\n",
      " 53. 65. 13.  7. 47. 22. 32. 19. 23. 31. 41. 37. 26. 29. 58. 33. 17.  3.\n",
      " 11. 86. 39. 59.  2. 74. 62. 94. 77. 91. 57. 81. 82. 67. 92. 88. 73.  1.\n",
      " 87. 61. 69. 95.]\n",
      "educ_4_label\n",
      "['Secondary' 'No/Primary' 'Post-sec / Short tertiary' 'University+' nan]\n",
      "age\n",
      "[ 35.  24.  18.  61.  55.  53.  22.  39.  49.  28.  29.  31.  45.  59.\n",
      "  54.  20.  64.  70.  76.  75.  32.  63.  62.  25.  26.  27.  21.  41.\n",
      "  50.  57.  46.  48.  36.  74.  23.  34.  43.  47.  51.  33.  71.  81.\n",
      "  52.  42.  82.  66.  38.  19.  40.  72.  60.  73.  69.  56.  37.  85.\n",
      "  65.  79.  77.  78.  68.  84.  67.  30.  58.  86.  91.  87.  44.  83.\n",
      "  80.  90.  92.  88.  nan  93.  89.  95.  94.  96.  17.  98. 102.  16.\n",
      "  15.  97. 101.]\n",
      "urban_rural\n",
      "['Suburban' 'Town' 'Rural' 'Urban' nan]\n",
      "spouse_work_status\n",
      "['NAP' 'Paid work' 'Retired' 'DK/No answer' 'Domestic work' 'Education'\n",
      " 'Sick/Disabled' 'Military/Community service' 'Other' 'Unemployed'\n",
      " 'Apprentice/Trainee']\n",
      "sex\n",
      "['Female' 'Male' nan]\n",
      "code_higher_income\n",
      "[nan]\n",
      "code_income_control\n",
      "[nan 'Financial control is shared'\n",
      " 'Financial control is with the respondent'\n",
      " 'Financial control is separate' 'Financial control is with the partner']\n",
      "hh_wrk_hrs\n",
      "[18.  8.  3.  5. 10. 28.  4. 20.  6. 17. 32. 15.  7. 13.  2. 42. 14. 49.\n",
      " 21. 12. 35. 60. 16.  9. 30.  1. 25.  0. 50. 34. 11. 40. 24. 26. 45. 22.\n",
      " nan 31. 71. 47. 19. 58. 95. 38. 39. 80. 85. 36. 70. 84. 90. 27. 48. 44.\n",
      " 29. 23. 56. 37. 63. 46. 33. 72. 68. 64. 55. 65. 82. 59. 88. 66. 78. 73.\n",
      " 62. 75. 54. 74. 52. 43. 76. 86. 67. 41. 53. 94. 91. 61. 83.]\n",
      "work_status\n",
      "['Paid work' 'Domestic work' 'Education' 'Unemployed' 'Retired'\n",
      " 'Sick/Disabled' 'DK/No answer' 'Other' 'Apprentice/Trainee'\n",
      " 'Military/Community service']\n",
      "marital\n",
      "['Divorced' 'Single' 'Married' 'Civil partnership' nan 'Widowed']\n",
      "wrk_hrs\n",
      "[35. 40. 20. 50. nan 30. 60. 38. 15. 45. 10. 78. 48.  2. 25. 32. 80. 36.\n",
      " 16. 44. 70. 42. 39.  3. 27. 24. 46.  8. 28. 31. 22. 33. 12. 47. 55. 19.\n",
      " 26. 13. 41.  7. 37.  5. 18. 65.  6. 11. 17. 23. 43. 95. 14. 21. 72. 34.\n",
      " 52. 58.  9. 29.  4. 54. 56. 90. 84. 63. 75. 57. 96. 51.  1. 64. 92. 71.\n",
      " 77. 81. 91. 83. 49. 86. 62. 66. 53. 68. 74. 76. 85. 59. 88. 67. 73. 82.\n",
      " 61. 93. 89.]\n",
      "educ_4_label\n",
      "[nan 'Post-sec / Short tertiary' 'Secondary' 'University+' 'No/Primary']\n",
      "age\n",
      "[ 46.  59.  28.  60.  43.  65.  24.  33.  72.  29.  42.  55.  66.  26.\n",
      "  76.  34.  63.  49.  27.  30.  54.  40.  74.  53.  73.  51.  35.  41.\n",
      "  19.  45.  61.  71.  64.  62.  36.  68.  57.  56.  70.  47.  23.  52.\n",
      "  69.  37.  77.  80.  20.  21.  39.  48.  38.  44.  22.  50.  79.  67.\n",
      "  31.  58.  75.  25.  83.  32.  88.  18.  92.  84.  17.  82.  78.  81.\n",
      "  89.  16.  86.  90.   9.  96.  93.  87.  91.  85.  94.  95.  98. 100.\n",
      "  97.  99.  15.]\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_2002_clean, df_2012_clean, df_2022_clean]\n",
    "cleaned_vars = [\n",
    "    \"urban_rural\",\n",
    "    \"spouse_work_status\",\n",
    "    \"sex\",\n",
    "    \"code_higher_income\",\n",
    "    \"code_income_control\",\n",
    "    \"hh_wrk_hrs\",\n",
    "    \"work_status\",\n",
    "    \"marital\",\n",
    "    \"wrk_hrs\",\n",
    "    \"educ_4_label\",\n",
    "    \"age\",\n",
    "]\n",
    "for df in dfs:\n",
    "    for var in cleaned_vars:\n",
    "        print(var)\n",
    "        print(df[var].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e46d4e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45762, 69)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2022_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "0225a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved clean dataframes to ../clean_csv/\n"
     ]
    }
   ],
   "source": [
    "df_2002_clean.to_csv(\"../new_clean_csv/2002_clean.csv\", index=False)\n",
    "df_2012_clean.to_csv(\"../new_clean_csv/2012_clean.csv\", index=False)\n",
    "df_2022_clean.to_csv(\"../new_clean_csv/2022_clean.csv\", index=False)\n",
    "\n",
    "print(\"✓ Saved clean dataframes to ../clean_csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1fb8d769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022: (45762, 69)\n",
      "2012: (61754, 69)\n",
      "2002: (46638, 69)\n"
     ]
    }
   ],
   "source": [
    "# Read the saved clean CSVs\n",
    "\n",
    "df_2002_clean = pd.read_csv(\"../new_clean_csv/2002_clean.csv\")\n",
    "print(f\"2022: {df_2022_clean.shape}\")\n",
    "\n",
    "df_2012_clean = pd.read_csv(\"../new_clean_csv/2012_clean.csv\")\n",
    "print(f\"2012: {df_2012_clean.shape}\")\n",
    "\n",
    "df_2022_clean = pd.read_csv(\"../new_clean_csv/2022_clean.csv\")\n",
    "print(f\"2002: {df_2002_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "cefb1a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v4_egal</th>\n",
       "      <th>v5_egal</th>\n",
       "      <th>v6_egal</th>\n",
       "      <th>v7_egal</th>\n",
       "      <th>v8_egal</th>\n",
       "      <th>v11_egal</th>\n",
       "      <th>v4_egal_z</th>\n",
       "      <th>v5_egal_z</th>\n",
       "      <th>v6_egal_z</th>\n",
       "      <th>v7_egal_z</th>\n",
       "      <th>...</th>\n",
       "      <th>LIFE_HAP</th>\n",
       "      <th>DIFF_CONC_WORK</th>\n",
       "      <th>HH_TIRED</th>\n",
       "      <th>HH_FAM</th>\n",
       "      <th>WORK_TIRED</th>\n",
       "      <th>HH_WEEKEND</th>\n",
       "      <th>COHAB</th>\n",
       "      <th>HOMPOP</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>CASEID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.357029</td>\n",
       "      <td>-0.614036</td>\n",
       "      <td>-0.676980</td>\n",
       "      <td>0.125346</td>\n",
       "      <td>...</td>\n",
       "      <td>Neither happy nor unhappy</td>\n",
       "      <td>Several times a month</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Several times a month</td>\n",
       "      <td>Mostly me</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 persons</td>\n",
       "      <td>Australia (AU)</td>\n",
       "      <td>1000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.105198</td>\n",
       "      <td>1.788120</td>\n",
       "      <td>1.680397</td>\n",
       "      <td>1.741713</td>\n",
       "      <td>...</td>\n",
       "      <td>Very happy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We decide together</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 persons</td>\n",
       "      <td>Australia (AU)</td>\n",
       "      <td>1000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>0.186683</td>\n",
       "      <td>-0.676980</td>\n",
       "      <td>0.125346</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly happy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mostly me</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 persons</td>\n",
       "      <td>Australia (AU)</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.357029</td>\n",
       "      <td>-0.614036</td>\n",
       "      <td>-1.462772</td>\n",
       "      <td>0.933529</td>\n",
       "      <td>...</td>\n",
       "      <td>Very happy</td>\n",
       "      <td>Never</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Never</td>\n",
       "      <td>We decide together</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 persons</td>\n",
       "      <td>Australia (AU)</td>\n",
       "      <td>1000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.536287</td>\n",
       "      <td>-0.614036</td>\n",
       "      <td>-0.676980</td>\n",
       "      <td>0.125346</td>\n",
       "      <td>...</td>\n",
       "      <td>Very happy</td>\n",
       "      <td>Once or twice a year</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once or twice a year</td>\n",
       "      <td>We decide together</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 persons</td>\n",
       "      <td>Australia (AU)</td>\n",
       "      <td>1000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46633</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>0.987401</td>\n",
       "      <td>0.894605</td>\n",
       "      <td>-0.682838</td>\n",
       "      <td>...</td>\n",
       "      <td>Very happy</td>\n",
       "      <td>Never</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Several times a month</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>3 persons</td>\n",
       "      <td>Taiwan (TW)</td>\n",
       "      <td>39093339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46634</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>-0.614036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.491022</td>\n",
       "      <td>...</td>\n",
       "      <td>Completely happy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1 person</td>\n",
       "      <td>Taiwan (TW)</td>\n",
       "      <td>39093340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46635</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>-0.614036</td>\n",
       "      <td>0.894605</td>\n",
       "      <td>-0.682838</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly happy</td>\n",
       "      <td>Once or twice a year</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once or twice a year</td>\n",
       "      <td>Mostly me</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5 persons</td>\n",
       "      <td>Taiwan (TW)</td>\n",
       "      <td>39093341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46636</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>0.987401</td>\n",
       "      <td>0.894605</td>\n",
       "      <td>0.933529</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly happy</td>\n",
       "      <td>Never</td>\n",
       "      <td>Never</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Several times a month</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>7 persons</td>\n",
       "      <td>Taiwan (TW)</td>\n",
       "      <td>39093342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46637</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.284456</td>\n",
       "      <td>0.987401</td>\n",
       "      <td>0.894605</td>\n",
       "      <td>-1.491022</td>\n",
       "      <td>...</td>\n",
       "      <td>Fairly happy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>1 person</td>\n",
       "      <td>Taiwan (TW)</td>\n",
       "      <td>39093344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46638 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       v4_egal  v5_egal  v6_egal  v7_egal  v8_egal  v11_egal  v4_egal_z  \\\n",
       "0          2.0      2.0      2.0      3.0      3.0       3.0  -1.357029   \n",
       "1          5.0      5.0      5.0      5.0      1.0       5.0   1.105198   \n",
       "2          4.0      3.0      2.0      3.0      2.0       4.0   0.284456   \n",
       "3          2.0      2.0      1.0      4.0      4.0       4.0  -1.357029   \n",
       "4          3.0      2.0      2.0      3.0      2.0       3.0  -0.536287   \n",
       "...        ...      ...      ...      ...      ...       ...        ...   \n",
       "46633      4.0      4.0      4.0      2.0      2.0       4.0   0.284456   \n",
       "46634      4.0      2.0      NaN      1.0      2.0       1.0   0.284456   \n",
       "46635      4.0      2.0      4.0      2.0      2.0       3.0   0.284456   \n",
       "46636      4.0      4.0      4.0      4.0      2.0       4.0   0.284456   \n",
       "46637      4.0      4.0      4.0      1.0      2.0       2.0   0.284456   \n",
       "\n",
       "       v5_egal_z  v6_egal_z  v7_egal_z  ...                   LIFE_HAP  \\\n",
       "0      -0.614036  -0.676980   0.125346  ...  Neither happy nor unhappy   \n",
       "1       1.788120   1.680397   1.741713  ...                 Very happy   \n",
       "2       0.186683  -0.676980   0.125346  ...               Fairly happy   \n",
       "3      -0.614036  -1.462772   0.933529  ...                 Very happy   \n",
       "4      -0.614036  -0.676980   0.125346  ...                 Very happy   \n",
       "...          ...        ...        ...  ...                        ...   \n",
       "46633   0.987401   0.894605  -0.682838  ...                 Very happy   \n",
       "46634  -0.614036        NaN  -1.491022  ...           Completely happy   \n",
       "46635  -0.614036   0.894605  -0.682838  ...               Fairly happy   \n",
       "46636   0.987401   0.894605   0.933529  ...               Fairly happy   \n",
       "46637   0.987401   0.894605  -1.491022  ...               Fairly happy   \n",
       "\n",
       "              DIFF_CONC_WORK  HH_TIRED HH_FAM             WORK_TIRED  \\\n",
       "0      Several times a month     Never    NaN  Several times a month   \n",
       "1                        NaN       NaN    NaN                    NaN   \n",
       "2                        NaN       NaN    NaN                    NaN   \n",
       "3                      Never     Never    NaN                  Never   \n",
       "4       Once or twice a year     Never    NaN   Once or twice a year   \n",
       "...                      ...       ...    ...                    ...   \n",
       "46633                  Never     Never    NaN  Several times a month   \n",
       "46634                    NaN       NaN    NaN                    NaN   \n",
       "46635   Once or twice a year     Never    NaN   Once or twice a year   \n",
       "46636                  Never     Never    NaN  Several times a month   \n",
       "46637                    NaN       NaN    NaN                    NaN   \n",
       "\n",
       "               HH_WEEKEND COHAB     HOMPOP         COUNTRY    CASEID  \n",
       "0               Mostly me   NaN  2 persons  Australia (AU)   1000001  \n",
       "1      We decide together   NaN  2 persons  Australia (AU)   1000002  \n",
       "2               Mostly me   NaN  2 persons  Australia (AU)   1000003  \n",
       "3      We decide together   NaN  2 persons  Australia (AU)   1000004  \n",
       "4      We decide together   NaN  2 persons  Australia (AU)   1000005  \n",
       "...                   ...   ...        ...             ...       ...  \n",
       "46633                 NaN    No  3 persons     Taiwan (TW)  39093339  \n",
       "46634                 NaN    No   1 person     Taiwan (TW)  39093340  \n",
       "46635           Mostly me   NaN  5 persons     Taiwan (TW)  39093341  \n",
       "46636                 NaN    No  7 persons     Taiwan (TW)  39093342  \n",
       "46637                 NaN    No   1 person     Taiwan (TW)  39093344  \n",
       "\n",
       "[46638 rows x 69 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2002_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "8021b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_vars = [\n",
    "    \"urban_rural\",\n",
    "    \"spouse_work_status\",\n",
    "    \"sex\",\n",
    "    \"code_higher_income\",\n",
    "    \"code_income_control\",\n",
    "    \"hh_wrk_hrs\",\n",
    "    \"work_status\",\n",
    "    \"marital\",\n",
    "    \"wrk_hrs\",\n",
    "    \"educ_4_label\",\n",
    "    \"age\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "8c1df537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== TOPBOT ====================\n",
      "==================== df_2002_clean ====================\n",
      "['5.0' '8.0' '6.0' '4.0' '7.0' nan '3.0' 'Lowest' '2.0' '9.0' 'Highest']\n",
      "==================== df_2012_clean ====================\n",
      "['05' '07' '08' '03' '06' '04' 'Lowest, Bottom, 01' \"Don't know\" '02'\n",
      " 'Highest, Top, 10' '09' 'No answer' 'Refused' 'Not available: GB,US']\n",
      "==================== df_2022_clean ====================\n",
      "['4. 04' '1. Lowest, Bottom, 01' '8. 08' '5. 05' '7. 07' '6. 06'\n",
      " \"-8. Don't know\" '10. Highest, Top, 10' '9. 09' '2. 02' '3. 03'\n",
      " '-9. No answer' '-1. CZ: Not available']\n",
      "==================== SPWRKHRS ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan '40 hours' '37.0' '10 hours' '45.0' '20 hours' '12.0' '55.0' '38.0'\n",
      " '30 hours' '42.0' '48.0' '50 hours' '58.0' '44.0' '25.0' '19.0' '60.0'\n",
      " '28.0' '35.0' '39.0' '8.0' '15 hours' '22.0' '43.0' '3.0' '16.0' '72.0'\n",
      " '80.0' '24.0' '70.0' '65.0' '96 hours and more' '41.0' '11.0' '18.0'\n",
      " '17.0' '56.0' '4.0' '6.0' '36.0' '34.0' '75.0' '46.0' '54.0' '90.0'\n",
      " '32.0' '47.0' '2 hours' '33.0' '26.0' '52.0' '64.0' '13.0' '27.0' '23.0'\n",
      " '21.0' '84.0' '53.0' '66.0' '51.0' '29.0' '14.0' '49.0' '57.0' '63.0'\n",
      " '85.0' '67.0' '5 hours' '31.0' '61.0' '7.0' '76.0' '71.0' '9.0' '1 hour'\n",
      " '77.0' '68.0' '86.0' '62.0' '78.0' '95.0' '91.0' '82.0' '69.0' '88.0'\n",
      " '89.0' '74.0' '92.0' '94.0']\n",
      "==================== df_2012_clean ====================\n",
      "['50' '64' 'NAP (code 0,2,3 (AR,VE: 0,3) in SPWORK); not available: BG,GB'\n",
      " '20' '48' '30' '60' '25' '12' '24' '82' '44' '96 hours or more' '40' '28'\n",
      " \"Don't know\" '75' '45' '54' '70' '35' '42' '73' '56' '55' '43' '33' '49'\n",
      " '36' '72' '80' '5' '63' '84' '65' '10' '47' '46' '66' '78' '51' '18' '8'\n",
      " '37' 'No answer' '41' '90' '15' '16' '38' '26' '14' '9' '39' '4' '2' '22'\n",
      " '23' '53' '93' '19' '32' '3' '6' '13' '21' '34' '11' '7' '52' '58' '94'\n",
      " '91' '77' '67' '57' '62' '59' '85' '68' 'Refused; TW: time varies' '76'\n",
      " '74' '88' '69' '81' '27' '61' '31' '29' '17' '87' '95' 'One hour' '86'\n",
      " '79' '71' '89']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP (code -4, 2 or 3 in SPWORK)' '40' '45' '50' '65' '-9. No answer'\n",
      " '39' '26' '37' '20' '47' '44' '15' '60' '30' '80' '35' '38' '25' '54'\n",
      " '55' '42' '32' '33' '75' '36' '28' '34' '52' '41' '22' '18' '46' '10'\n",
      " '43' '24' '27' '53' '70' '23' '16' '48' '14' '21' '64' '84' '8' '56' '72'\n",
      " '5' '96. 96 hours or more' '12' '3' '17' '6' '4' '9' '2' '11' '13' '31'\n",
      " '1. One hour' '58' '29' '62' '77' '7' '66' '90' '19' '69' '91'\n",
      " \"-8. Don't know\" '68' '57' '89. 89 hours; US: 89 or more hours' '85' '63'\n",
      " '49' '59' '51' '78' '-6. IL: Invalid answers' '74' '95' '92' '76' '87'\n",
      " '61']\n",
      "==================== C_ALPHAN ====================\n",
      "==================== df_2002_clean ====================\n",
      "['AU' 'DE-W' 'DE-E' 'GB-GBN' 'GB-NIR' 'US' 'AT' 'HU' 'IE' 'NL' 'NO' 'SE'\n",
      " 'CZ' 'SI' 'PL' 'BG' 'RU' 'NZ' 'PH' 'IL' 'JP' 'ES' 'LV' 'SK' 'FR' 'CY'\n",
      " 'PT' 'CL' 'DK' 'CH' 'BE-FLA' 'BR' 'FI' 'MX' 'TW']\n",
      "==================== df_2012_clean ====================\n",
      "['AR' 'AU' 'AT' 'BG' 'CA' 'CL' 'CN' 'TW' 'HR' 'CZ' 'DK' 'FI' 'FR' 'HU'\n",
      " 'IS' 'IN' 'IE' 'IL' 'JP' 'KR' 'LV' 'LT' 'MX' 'NL' 'NO' 'PH' 'PL' 'RU'\n",
      " 'SK' 'SI' 'ZA' 'ES' 'SE' 'CH' 'TR' 'US' 'VE' 'BE' 'DE' 'PT' 'GB-GBN']\n",
      "==================== df_2022_clean ====================\n",
      "['AT' 'AU' 'BG' 'CH' 'CZ' 'DE' 'DK' 'ES' 'FI' 'FR' 'GR' 'HR' 'HU' 'IL'\n",
      " 'IN' 'IS' 'IT' 'JP' 'LT' 'NL' 'NO' 'NZ' 'PH' 'PL' 'RU' 'SE' 'SI' 'SK'\n",
      " 'TH' 'TW' 'US' 'ZA']\n",
      "==================== LIVWOMAR ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Neither agree nor disagree' 'Agree' 'Disagree' 'Strongly disagree'\n",
      " 'Strongly agree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "['Agree' 'Disagree' 'Neither agree nor disagree' 'Strongly agree'\n",
      " 'Strongly disagree' \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['1. Strongly agree' '2. Agree' '3. Neither agree nor disagree'\n",
      " \"-8. Can't choose\" '5. Strongly disagree' '4. Disagree' '-9. No answer']\n",
      "==================== WWYKS ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Work part-time' 'Work full-time' 'Stay at home' nan\n",
      " 'TW:women shld decide']\n",
      "==================== df_2012_clean ====================\n",
      "['Work part-time' 'Stay at home' 'Work full-time'\n",
      " \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'TW: women should decide']\n",
      "==================== df_2022_clean ====================\n",
      "['1. Work full-time' '2. Work part-time' '3. Stay at home'\n",
      " \"-8. Can't choose\" '-9. No answer; PL: Refused'\n",
      " '4. TW: Women should decide']\n",
      "==================== WWYKUS ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Stay at home' 'Work full-time' 'Work part-time' nan\n",
      " 'TW:women shld decide']\n",
      "==================== df_2012_clean ====================\n",
      "['Stay at home' 'Work part-time' 'Work full-time'\n",
      " \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'TW: women should decide']\n",
      "==================== df_2022_clean ====================\n",
      "['1. Work full-time' '2. Work part-time' '3. Stay at home'\n",
      " \"-8. Can't choose\" '-9. No answer; PL: Refused'\n",
      " '4. TW: Women should decide']\n",
      "==================== SP_DEGREE ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Lowest formal qualification' nan 'Above higher secondary level'\n",
      " 'Higher secondary completed' 'Above lowest qualification'\n",
      " 'University degree completed' 'No formal qualification']\n",
      "==================== df_2012_clean ====================\n",
      "['Lower level tertiary, first stage (also technical schools at a tertiary level)'\n",
      " 'No formal education'\n",
      " 'NAP, no partner (code 3 (AR,CZ,PT: 3,7; IS: 3,7,9) in PARTLIV; TW,US: 3-6 in MARITAL)'\n",
      " 'Primary school'\n",
      " 'Upper secondary (programs that allow entry to university'\n",
      " 'Upper level tertiary (Master, Dr.)' \"No answer, don't know, refused\"\n",
      " 'Lower secondary (secondary completed does not allow entry to university: obligatory school)'\n",
      " 'Post secondary, non-tertiary (other upper secondary programs toward labour market or technical formation)'\n",
      " 'Not available: AT,BG,CL,GB,IE,IL,LV,NO,PH,RU,ZA']\n",
      "==================== df_2022_clean ====================\n",
      "[\"-4. NAP (c. 3,-7 PARTLIV; IT, RU: c. 2,3,-7 PARTLIV; AT: c. 3-6 MARITAL); NO: sep. instruction 'If no partner, skip question\"\n",
      " '4. Post secondary, non-tertiary' '8. PhD, Post tertiary specialization'\n",
      " '5. Short-cycle tertiary' '3. Upper secondary'\n",
      " '7. Upper level tertiary, MA'\n",
      " '-9. No answer, not classifiable; AT, ES, TW: DK/NA; CH: DK/ not codable; PL: Hard to say/NA'\n",
      " '2. Lower secondary' '6. Lower level tertiary, BA' '1. Primary education'\n",
      " '0. No (formal) education, incomplete primary'\n",
      " '-1. DK, NZ, ZA: Not available']\n",
      "==================== MOMORFAF ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Neither agree nor disagree' 'Strongly agree' 'Agree' 'Strongly disagree'\n",
      " 'Disagree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "[nan]\n",
      "==================== df_2022_clean ====================\n",
      "['3. Mothers and fathers are equally suited'\n",
      " '2. Mothers are somewhat better suited'\n",
      " '1. Mothers are much better suited' \"-8. Can't choose\"\n",
      " '4. Fathers are somewhat better suited'\n",
      " '5. Fathers are much better suited' '-9. No answer']\n",
      "==================== MEWH ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Neither agree nor disagree' 'Strongly disagree' 'Disagree' 'Agree'\n",
      " 'Strongly agree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "['Agree' 'Disagree' 'Neither agree nor disagree' 'Strongly agree'\n",
      " \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'Strongly disagree' 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['5. Strongly disagree' '2. Agree' '3. Neither agree nor disagree'\n",
      " '1. Strongly agree' '4. Disagree' \"-8. Can't choose\" '-9. No answer']\n",
      "==================== HHTODD ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan '1 child' '2 children' '3 children' '4 children' '5 children'\n",
      " '8 children' '6 children' '9 children' '7 children']\n",
      "==================== df_2012_clean ====================\n",
      "['No toddlers' '2 toddlers' 'One toddler' '3' 'No answer' '4' '5'\n",
      " 'Refused' '6' '7' '10 toddlers' '8' '9'\n",
      " 'NAP (Code 0 in HOMPOP); not available: TR']\n",
      "==================== df_2022_clean ====================\n",
      "['0. No children below [age of school entry]'\n",
      " '1. One child below [age of school entry]'\n",
      " '-9. No answer; AU: No answer and no children below 5'\n",
      " '2. Two children below [age of school entry]'\n",
      " '3. Three children below [age of school entry]'\n",
      " '4. Four children below [age of school entry]'\n",
      " '-4. NAP (Not a private household)'\n",
      " '5. Five children below [age of school entry]; SE: Five or more'\n",
      " '7. Seven children below [age of school entry]' '-1. NL: Not available'\n",
      " '6. Six children below [age of school entry]']\n",
      "==================== HHCHILDR ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan '2 children' '1 child' '3 children' '4 children' '5 children'\n",
      " '6 children' '8 children' '7 children' '10 children' '9 children']\n",
      "==================== df_2012_clean ====================\n",
      "['One child' 'No children' '5' '2 children' '4' '3' '7' 'No answer'\n",
      " '21 children' '18' '8' '6' 'Refused' '10' '9' '12' '15' '14' '11'\n",
      " 'NAP (Code 0 in HOMPOP); not available: TR']\n",
      "==================== df_2022_clean ====================\n",
      "['1. One child [school entry age] to 17'\n",
      " '0. No children [school entry age] to 17'\n",
      " '2. Two children [school entry age] to 17'\n",
      " '-9. No answer; AU: No answer and no children between 5 to 17'\n",
      " '3. Three children [school entry age] to 17'\n",
      " '4. Four children [school entry age] to 17'\n",
      " '5. Five children [school entry age] to 17; SE: Five or more'\n",
      " '7. Seven children [school entry age] to 17'\n",
      " '6. Six children [school entry age] to 17'\n",
      " '-4. NAP (Not a private household)'\n",
      " '9. Nine children [school entry age] to 17'\n",
      " '8. Eight children [school entry age] to 17']\n",
      "==================== HHADULT ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan '2 persons' '1 person' '3 persons' '4 persons' '5 persons'\n",
      " '6 persons' '7 persons' '12 persons' '8 persons' '9 persons' '11 persons'\n",
      " '10 persons' '13,DK:13 or more persons' '15 persons' '17 persons']\n",
      "==================== df_2012_clean ====================\n",
      "[nan]\n",
      "==================== df_2022_clean ====================\n",
      "['1. One adult (only respondent)' '4. Four adults' '2. Two adults'\n",
      " '-9. No answer' '3. Three adults' '6. Six adults'\n",
      " '5. Five adults; SE: Five or more' '8. Eight adults' '7. Seven adults'\n",
      " '9. Nine adults' '-4. NAP (Not a private household)' '11. Eleven adults'\n",
      " '15. Fifteen adults' '22. Twenty-two adults' '10. Ten adults'\n",
      " '12. Twelve adults' '14. Fourteen adults' '16. Sixteen adults'\n",
      " '0. ZA: No adult in household']\n",
      "==================== FAM_DIF ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Once or twice a year' nan 'Several times a month' 'Never'\n",
      " 'Several times a week']\n",
      "==================== df_2012_clean ====================\n",
      "['Never'\n",
      " \"Doesn't apply: no job, no family responsibilities (HR,LT,MX,PL,RU,TW,VE: 2 or 3 in WORK)\"\n",
      " 'Several times a month' 'Once or twice' 'Several times a week'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['1. Several times a week' '4. Never' '3. Once or twice'\n",
      " '2. Several times a month'\n",
      " \"-4. Doesn't apply/no job; TW: NAP (code 2,3,5,6,7 in MAINSTAT)\"\n",
      " '-9. No answer; LT: NA/DK/Hard to say' \"-8. DE, TW: Can't choose\"\n",
      " '-1. DK: Not available']\n",
      "==================== SHARE_HH ====================\n",
      "==================== df_2002_clean ====================\n",
      "['I do a bit more than my fair share' 'I do roughly my fair share'\n",
      " 'I do a bit less than my fair share' 'I do much more than my fair share'\n",
      " nan 'I do much less than my fair share' 'PH: None of above']\n",
      "==================== df_2012_clean ====================\n",
      "['I do roughly my fair share' 'I do much more than my fair share'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'I do a bit more than my fair share' 'I do a bit less than my fair share'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\"\n",
      " 'I do much less than my fair share' 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '2. I do a bit more than my fair share'\n",
      " '1. I do much more than my fair share' '3. I do roughly my fair share'\n",
      " \"-9. No answer; AT, ES, HU, SI: Can't Ch/NA; IS: NA/DK; LT: NA/DK/Hard to say; PL: Hard to say/NA\"\n",
      " '4. I do a bit less than my fair share'\n",
      " '5. I do much less than my fair share']\n",
      "==================== HW_FULFIL ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Neither agree nor disagree' 'Strongly agree' 'Agree' 'Disagree'\n",
      " 'Strongly disagree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "['Disagree' 'Agree' 'Neither agree nor disagree' 'Strongly agree'\n",
      " 'Strongly disagree' \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['3. Neither agree nor disagree' '1. Strongly agree'\n",
      " '5. Strongly disagree' \"-8. Can't choose\" '4. Disagree' '2. Agree'\n",
      " '-9. No answer']\n",
      "==================== WO_WANT ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Neither agree nor disagree' 'Strongly disagree' 'Disagree' 'Agree'\n",
      " 'Strongly agree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "['Agree' 'Neither agree nor disagree' 'Strongly agree' 'Disagree'\n",
      " \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'Strongly disagree' 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['5. Strongly disagree' '2. Agree' '1. Strongly agree' '4. Disagree'\n",
      " \"-8. Can't choose\" '3. Neither agree nor disagree' '-9. No answer']\n",
      "==================== WW_FAM_SUFFER ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Agree' 'Strongly disagree' 'Strongly agree' 'Neither agree nor disagree'\n",
      " 'Disagree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "['Agree' 'Strongly agree' 'Disagree' 'Neither agree nor disagree'\n",
      " 'Strongly disagree' \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['5. Strongly disagree' '2. Agree' '1. Strongly agree' '4. Disagree'\n",
      " '3. Neither agree nor disagree' \"-8. Can't choose\" '-9. No answer']\n",
      "==================== WW_CHILD_SUFFER ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Agree' 'Strongly disagree' 'Neither agree nor disagree' nan\n",
      " 'Strongly agree' 'Disagree']\n",
      "==================== df_2012_clean ====================\n",
      "['Agree' 'Neither agree nor disagree' 'Strongly agree' 'Disagree'\n",
      " 'Strongly disagree' \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['5. Strongly disagree' '1. Strongly agree' '2. Agree' '4. Disagree'\n",
      " '3. Neither agree nor disagree' \"-8. Can't choose\" '-9. No answer']\n",
      "==================== WW_WARM ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Disagree' 'Strongly agree' 'Agree' 'Neither agree nor disagree'\n",
      " 'Strongly disagree' nan]\n",
      "==================== df_2012_clean ====================\n",
      "['Disagree' 'Agree' 'Strongly agree' 'Neither agree nor disagree'\n",
      " 'Strongly disagree' \"Can't choose, CA:+NA, KR:DK,ref., NL:DK\" 'No answer'\n",
      " 'NAP: ES']\n",
      "==================== df_2022_clean ====================\n",
      "['1. Strongly agree' '4. Disagree' '5. Strongly disagree'\n",
      " '3. Neither agree nor disagree' \"-8. Can't choose\" '2. Agree'\n",
      " '-9. No answer']\n",
      "==================== DIV_HH_COOK ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Always me,PL:the woman' 'About equal o both together'\n",
      " 'Usually my spouse,partner,PL:the man'\n",
      " 'Always my spouse,partner,PL:the man' nan 'Usually me,PL:the woman'\n",
      " 'Done by a third person']\n",
      "==================== df_2012_clean ====================\n",
      "['About equal or both together' 'Always me'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'Usually my spouse/ partner' 'Is done by a third person'\n",
      " 'Always my spouse/ partner' 'Usually me'\n",
      " \"Can't choose, KR:DK,ref., NL:DK\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '3. About equal or both together' '2. Usually me'\n",
      " '4. Usually my spouse/partner' '1. Always me'\n",
      " '5. Always my spouse/partner' \"-8. Can't choose\"\n",
      " '6. Is done by a third person'\n",
      " \"-9. No answer; ES: Can't choose/ No answer; TW: NAP (e.g. never have meals together at home)\"]\n",
      "==================== DIV_HH_CLEAN ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Usually me,PL:the woman' 'About equal o both together'\n",
      " 'Usually my spouse,partner,PL:the man' 'Always me,PL:the woman'\n",
      " 'Always my spouse,partner,PL:the man' nan 'Done by a third person']\n",
      "==================== df_2012_clean ====================\n",
      "['About equal or both together' 'Always me'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'Usually my spouse/ partner' 'Is done by a third person'\n",
      " 'Always my spouse/ partner' 'Usually me'\n",
      " \"Can't choose, KR:DK,ref., NL:DK\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '2. Usually me' '3. About equal or both together'\n",
      " '6. Is done by a third person' '1. Always me'\n",
      " '4. Usually my spouse/partner' '5. Always my spouse/partner'\n",
      " \"-8. Can't choose\" \"-9. No answer; ES: Can't choose/ No answer\"]\n",
      "==================== DIV_HH_GROC ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Always me,PL:the woman' 'About equal o both together'\n",
      " 'Usually me,PL:the woman' 'Usually my spouse,partner,PL:the man' nan\n",
      " 'Always my spouse,partner,PL:the man' 'Done by a third person']\n",
      "==================== df_2012_clean ====================\n",
      "['Always me'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'About equal or both together' 'Usually me' 'Usually my spouse/ partner'\n",
      " 'Always my spouse/ partner' 'Is done by a third person'\n",
      " \"Can't choose, KR:DK,ref., NL:DK\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '1. Always me' '4. Usually my spouse/partner'\n",
      " '3. About equal or both together' '2. Usually me' \"-8. Can't choose\"\n",
      " '5. Always my spouse/partner' '6. Is done by a third person'\n",
      " \"-9. No answer; ES: Can't choose/ No answer\"]\n",
      "==================== DIV_HH_CARE ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Always me,PL:the woman' 'About equal o both together'\n",
      " 'Usually my spouse,partner,PL:the man' nan 'Usually me,PL:the woman'\n",
      " 'Done by a third person' 'Always my spouse,partner,PL:the man']\n",
      "==================== df_2012_clean ====================\n",
      "['About equal or both together' 'Always me'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'Always my spouse/ partner' \"Can't choose, KR:DK,ref., NL:DK\"\n",
      " 'Usually me' 'Usually my spouse/ partner' 'Is done by a third person'\n",
      " 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '4. Usually my spouse/partner' '1. Always me'\n",
      " '3. About equal or both together' '2. Usually me' \"-8. Can't choose\"\n",
      " '6. Is done by a third person' '5. Always my spouse/partner'\n",
      " \"-9. No answer; ES: Can't choose/ No answer; TW: NAP (e.g. no sick people to be cared for)\"]\n",
      "==================== DIV_HH_LAUND ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Always me,PL:the woman' 'About equal o both together'\n",
      " 'Always my spouse,partner,PL:the man'\n",
      " 'Usually my spouse,partner,PL:the man' nan 'Usually me,PL:the woman'\n",
      " 'Done by a third person']\n",
      "==================== df_2012_clean ====================\n",
      "['About equal or both together' 'Always me'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3, AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'Usually my spouse/ partner' 'Always my spouse/ partner' 'Usually me'\n",
      " 'Is done by a third person' \"Can't choose, KR:DK,ref., NL:DK\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '2. Usually me' '1. Always me' '4. Usually my spouse/partner'\n",
      " '3. About equal or both together' '5. Always my spouse/partner'\n",
      " '6. Is done by a third person' \"-8. Can't choose\"\n",
      " \"-9. No answer; ES: Can't choose/ No answer\"]\n",
      "==================== SP_HH_FAM ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan]\n",
      "==================== df_2012_clean ====================\n",
      "['56' 'None, no hour'\n",
      " 'NAP, no partner (3 (AR,AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " '84' '7' '30' '14' '28' '70' '10' '95 hours and more' '45' '50'\n",
      " 'No answer, CA: no answer, refused' '35' '80' '21' '12' '2 hours' '20'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" '15' '18' '38'\n",
      " '25' '22' '4' '40' '42' '8' '60' '6' '3' '24' '36' '9' '54' '5' '16' '32'\n",
      " '48' '1 hour or less than 1 hour' '44' '53' '19' '27' '68' '90' '72' '26'\n",
      " '65' '89' '49' '46' '52' '13' '55' '34' '58' '39' '17' '11' '23' '47'\n",
      " '64' '88' '31' '63' '43' '33' '74' '78' '73' '62' '37' '29' '94' '91'\n",
      " '77' '69' '41' '76' '75' '59' '83' '57' '51' '82' '85' '86' '66' '92'\n",
      " '61']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '2' '20' '28' '9' '30' '3' '75' '6' '5' '0. None, no hours' '1. 1 hour'\n",
      " '7' '35' '10' '95. 95 hours and more' '50' '15' '40' '4' '21' '24' '70'\n",
      " '14' '16' '60' '12' '80' '8' '25' '23' '55' '18' '56' '36' '90' '59' '45'\n",
      " '37' '32' '41' '13' '38' '46'\n",
      " \"-9. No answer; HR, IS: DK/NA; ES, HU, TW: Can't Ch/NA; LT: NA/DK/Hard to say; PL: Hard to say/NA\"\n",
      " '22' '58' '34' '11' '44' '48' '29' '27' '84' '65' '53' '17' '64' '54'\n",
      " '85' '57' '19' '52' '39' '73' '42' '26' '72' '68' '43' '76' '49' '78'\n",
      " '61' '63' '33' '31' '62' '88' '91' '74' '77' '82' '66' '86' '47' '87'\n",
      " '79']\n",
      "==================== SP_HH ====================\n",
      "==================== df_2002_clean ====================\n",
      "['2 hrs' '5.0' '14.0' '28.0' '4.0' '24.0' '30.0' '10.0' nan '16.0' '15.0'\n",
      " '7.0' '50.0' '12.0' '56.0' '8.0' '20.0' '11.0' '18.0' '25.0' '3.0' '21.0'\n",
      " '40.0' '6.0' '35.0' 'None, no hour' '17.0' '1 hour or less than 1 hr'\n",
      " '70.0' '45.0' '60.0' '42.0' '9.0' '36.0' '54.0' '48.0' '49.0' '32.0'\n",
      " '55.0' '46.0' '26.0' '80.0' '90.0' '22.0' '19.0' '13.0' '65.0' '33.0'\n",
      " '63.0' '84.0' '44.0' '23.0' '38.0' '29.0' '34.0' '72.0'\n",
      " '95 hrs a more,US:96+,BR:98+' '64.0' '76.0' '66.0' '85.0' '27.0' '41.0'\n",
      " '75.0' '37.0' '39.0' '43.0' '58.0' '51.0' '91.0' '31.0' '59.0' '71.0'\n",
      " '52.0' '47.0' '74.0' '77.0' '93.0' '73.0' '69.0' '62.0' '68.0' '88.0']\n",
      "==================== df_2012_clean ====================\n",
      "['56' 'None, no hours'\n",
      " 'NAP, no partner (3 (AR,AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " '84' '20' '40' '14' '28' '15' '16' '70' '10' '35' '36' '4' '34' '7' '5'\n",
      " '30' '12' '21' '2 hours' '3 hours'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\"\n",
      " '1 hour or less than 1 hour' 'No answer, CA: no answer, refused' '22' '8'\n",
      " '17' '18' '25' '50' '48' '24' '42' '6' '60' '80' '45' '95 hours and more'\n",
      " '9' '54' '94' '65' '32' '49' '46' '90' '26' '38' '13' '11' '58' '55' '29'\n",
      " '33' '23' '27' '64' '63' '52' '47' '68' '31' '72' '37' '77' '51' '43'\n",
      " '69' '19' '62' '71' '75' '44' '74' '85' '86' '66' '92' '78' '76' '91'\n",
      " '81' '57' '41' '39' '61']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '2.0' '8.0' '5.0' '16.0' '1. 1 hour' '20.0' '4.0' '25.0'\n",
      " '0. None, no hours' '24.0' '14.0' '42.0' '3.0' '15.0' '12.0' '30.0'\n",
      " '35.0' '7.0' '45.0' '40.0' '10.0' '6.0' '11.0' '21.0' '18.0' '22.0' '9.0'\n",
      " '59.0' '13.0' '28.0' '32.0' '50.0' '60.0'\n",
      " \"-9. No answer; HR, IS: DK/NA; ES, HU, TW: Can't Ch/NA; LT: NA/DK/Hard to say; PL: Hard to say/NA\"\n",
      " '19.0' '29.0' '65.0' '36.0' '80.0' '90.0' '72.0' '48.0' '26.0' '56.0'\n",
      " '27.0' '23.0' '17.0' '54.0' '95. 95 hours and more' '55.0' '34.0' '69.0'\n",
      " '70.0' '44.0' '49.0' '52.0' '46.0' '38.0' '37.0' '31.0' '84.0' '68.0'\n",
      " '64.0' '88.0' '66.0' '75.0' '58.0' '43.0' '63.0' '41.0' '62.0' '51.0'\n",
      " '74.0' '33.0' '39.0' '82.0' '3.5' '61.0' '47.0' '76.0']\n",
      "==================== LIFE_HAP ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Neither happy nor unhappy' 'Very happy' 'Fairly happy'\n",
      " 'Completely happy' 'Very unhappy' 'Fairly unhappy' nan\n",
      " 'Completely unhappy']\n",
      "==================== df_2012_clean ====================\n",
      "['Very happy' 'Fairly unhappy' 'Completely happy' 'Fairly happy'\n",
      " 'Very unhappy' 'Neither happy nor unhappy' \"Can't choose, NL:Don't know\"\n",
      " 'No answer' 'Completely unhappy']\n",
      "==================== df_2022_clean ====================\n",
      "['3. Fairly happy' '1. Completely happy' '4. Neither happy nor unhappy'\n",
      " '2. Very happy' '6. Very unhappy' \"-8. Can't choose\" '5. Fairly unhappy'\n",
      " '7. Completely unhappy' '-9. No answer']\n",
      "==================== DIFF_CONC_WORK ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Several times a month' nan 'Never' 'Once or twice a year'\n",
      " 'Several times a week']\n",
      "==================== df_2012_clean ====================\n",
      "['Several times a month'\n",
      " \"Doesn't apply: no job, no family responsibilities (HR,LT,MX,PL,RU,TW,VE: 2 or 3 in WORK)\"\n",
      " 'Never' 'Once or twice' 'Several times a week'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['2. Several times a month' '4. Never' '3. Once or twice'\n",
      " \"-4. Doesn't apply/no job; TW: NAP (code 2,3,5,6,7 in MAINSTAT)\"\n",
      " '1. Several times a week' '-9. No answer; LT: NA/DK/Hard to say'\n",
      " \"-8. DE: Can't choose\"\n",
      " '-1. DK: Not available; US: Not available for respondents who used the Spanish questionnaire version']\n",
      "==================== HH_TIRED ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Never' nan 'Once or twice a year' 'Several times a month'\n",
      " 'Several times a week']\n",
      "==================== df_2012_clean ====================\n",
      "['Never'\n",
      " \"Doesn't apply: no job, no family responsibilities (HR,LT,MX,PL,RU,TW,VE: 2 or 3 in WORK)\"\n",
      " 'Several times a month' 'Once or twice' 'Several times a week'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['3. Once or twice' '4. Never'\n",
      " \"-4. Doesn't apply/no job; TW: NAP (code 2,3,5,6,7 in MAINSTAT)\"\n",
      " '2. Several times a month' '1. Several times a week'\n",
      " '-9. No answer; LT: NA/DK/Hard to say' \"-8. DE: Can't choose\"\n",
      " '-1. DK: Not available']\n",
      "==================== HH_FAM ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan]\n",
      "==================== df_2012_clean ====================\n",
      "['14' '84' 'None, no hours, does not apply, IN: no hours, no partner' '30'\n",
      " '48' '56' '22' '40' '21' '28' '70' '10' '95 hours and more' '20'\n",
      " 'No answer, CA: no answer, refused' '42' '16' '36' '12' '15' '50' '63'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" '24' '8' '60' '6'\n",
      " '25' '5' '3 hours' '4' '18' '7' '35' '49' '80' '54' '9' '13' '77'\n",
      " '2 hours' '46' '75' '57' '1 hour or less than 1 hour' '72' '52' '34' '45'\n",
      " '38' '90' '26' '44' '11' '66' '68' '89' '78' '65' '64' '69' '94' '53'\n",
      " '74' '81' '29' '51' '39' '92' '59' '55' '88' '33' '32' '17' '58' '31'\n",
      " '85' '27' '37' '19' '82' '62' '91' '86' '43' '23' '76' '47' '41' '73'\n",
      " '67' '83' '61']\n",
      "==================== df_2022_clean ====================\n",
      "['30' '0. None, no hours' '3' '4' '20' '9' '1. 1 hour' '75' '5' '10' '2'\n",
      " '40' '70' '7' '63' '8' '95. 95 hours and more' '49' '25' '21' '80' '90'\n",
      " '6' '14' '17' '65' '60' '24' '36' '18' '84' '50' '15' '45' '16' '32' '68'\n",
      " '44' '48' '55' '12' '42' '35' '54' '64' '51' '77' '72' '56' '28' '26'\n",
      " \"-9. No answer; HR, IS: DK/NA; ES, HU, TW: Can't Ch/NA; LT: NA/DK/Hard to say; PL: Hard to say/NA\"\n",
      " '22' '29' '41' '11' '19' '66' '34' '38' '27' '13' '53' '85' '62' '31'\n",
      " '78' '39' '33' '47' '61' '46' '57' '91' '52' '74' '37' '88' '23' '82'\n",
      " '58' '73' '43' '71' '92' '69' '94' '93' '59' '76' '81' '86' '87' '83'\n",
      " '67']\n",
      "==================== WORK_TIRED ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Several times a month' nan 'Never' 'Once or twice a year'\n",
      " 'Several times a week']\n",
      "==================== df_2012_clean ====================\n",
      "['Once or twice'\n",
      " \"Doesn't apply: no job, no family responsibilities (HR,LT,MX,PL,RU,TW,VE: 2 or 3 in WORK)\"\n",
      " 'Several times a month' 'Never' 'Several times a week'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['1. Several times a week' '3. Once or twice' '2. Several times a month'\n",
      " \"-4. Doesn't apply/no job; TW: NAP (code 2,3,5,6,7 in MAINSTAT)\"\n",
      " '4. Never' '-9. No answer; LT: NA/DK/Hard to say' \"-8. DE: Can't choose\"\n",
      " '-1. DK: Not available']\n",
      "==================== HH_WEEKEND ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Mostly me' 'We decide together' nan 'Sometimes me,sometimes spouse'\n",
      " 'Mostly my spouse,partner' 'Someone else']\n",
      "==================== df_2012_clean ====================\n",
      "['We decide together'\n",
      " 'NAP, no partner (3 (AT,BE,CH,CL,ES,IS,KR,NL,NO,PL,SK,US:2,3; AR,BG,CZ,PT:2,3,7;IL:3,7) in PARTLIV;TW:3-6 in MARITAL)'\n",
      " 'Mostly my spouse/ partner' 'Sometimes me/ sometimes spouse, partner'\n",
      " 'Mostly me' 'Someone else'\n",
      " \"Don't know, BG: can't choose, KR: don't know, refused\" 'No answer']\n",
      "==================== df_2022_clean ====================\n",
      "['-4. NAP, no partn. or not liv. with part. (c.2,3,-7 PARTLIV; US: c.2 (if neither married nor cohabit. with partner), 3 PARTL'\n",
      " '1. Always me' '3. About equal or both together' '2. Usually me'\n",
      " '4. Usually my spouse/partner' '5. Always my spouse/partner'\n",
      " \"-8. Can't choose\" \"-9. No answer; ES: Can't choose/ No answer\"\n",
      " '6. Is done by a third person']\n",
      "==================== COHAB ====================\n",
      "==================== df_2002_clean ====================\n",
      "[nan 'Yes' 'No']\n",
      "==================== df_2012_clean ====================\n",
      "['Yes, have partner; live in same household' 'No partner'\n",
      " \"Yes, have partner; don't live in same household\" 'Refused' 'No answer'\n",
      " 'Not available: DK,GB,TW']\n",
      "==================== df_2022_clean ====================\n",
      "['3. No partner' '1. Yes, have partner; live in same household'\n",
      " \"2. Yes, have partner; don't live in same household\" '-9. No answer'\n",
      " '-7. Refused']\n",
      "==================== HOMPOP ====================\n",
      "==================== df_2002_clean ====================\n",
      "['2 persons' '1 person' '5 persons' '4 persons' '3 persons' '6 persons'\n",
      " '8 persons or more' '7 persons' '10 persons' '9 persons' nan '15.0'\n",
      " '23.0' '12 persons' '11 persons' '14.0' '13, DK:13 or more persons'\n",
      " '20.0' '16.0' '17.0' '21.0' '18.0' '19.0' '24.0']\n",
      "==================== df_2012_clean ====================\n",
      "['3' '5' '4' '2 persons' '7' '8' '6' '9' '10'\n",
      " 'One person (only respondent)' '11' '12' 'No answer' '25' '35' '14' '13'\n",
      " '16' '15' '18' 'Not a private household; not available: TR' '20' '19'\n",
      " '17' '22' '37' '23' '21' '26' '24' '27' '32']\n",
      "==================== df_2022_clean ====================\n",
      "['2. Two persons' '1. One person (only respondent)' '4. Four persons'\n",
      " '3. Three persons' '-9. No answer' '5. Five persons' '6. Six persons'\n",
      " '7. Seven persons' '9. Nine persons' '8. Eight persons'\n",
      " '21. Twenty-one persons' '-4. NAP (Not a private household)'\n",
      " '15. Fifteen persons' '10. Ten persons' '12. Twelve persons'\n",
      " '17. Seventeen persons' '14. Fourteen persons' '11. Eleven persons'\n",
      " '13. Thirteen persons' '20. Eighteen persons' '16. Sixteen persons'\n",
      " '28. Twenty-eight persons' '22. Twenty-two persons'\n",
      " '18. Eighteen persons']\n",
      "==================== COUNTRY ====================\n",
      "==================== df_2002_clean ====================\n",
      "['Australia (AU)' 'Germany (West) (DE-W)' 'Germany (East) (DE-E)'\n",
      " 'Great Britain (GB-GBN)' 'Northern Ireland (GB-NIR)' 'United States (US)'\n",
      " 'Austria (AT)' 'Hungary (HU)' 'Ireland (IE)' 'Netherlands (NL)'\n",
      " 'Norway (NO)' 'Sweden (SE)' 'Czech Republic (CZ)' 'Slovenia (SI)'\n",
      " 'Poland (PL)' 'Bulgaria (BG)' 'Russia (RU)' 'New Zealand (NZ)'\n",
      " 'Philippines (PH)' 'Israel (IL)' 'Japan (JP)' 'Spain (ES)' 'Latvia (LV)'\n",
      " 'Slovak Republic (SK)' 'France (FR)' 'Cyprus (CY)' 'Portugal (PT)'\n",
      " 'Chile (CL)' 'Denmark (DK)' 'Switzerland (CH)'\n",
      " 'Belgium/ Flanders (BE-FLA)' 'Brazil (BR)' 'Finland (FI)' 'Mexico (MX)'\n",
      " 'Taiwan (TW)']\n",
      "==================== df_2012_clean ====================\n",
      "['AR-Argentina' 'AU-Australia' 'AT-Austria' 'BG-Bulgaria' 'CA-Canada'\n",
      " 'CL-Chile' 'CN-China' 'TW-Taiwan' 'HR-Croatia' 'CZ-Czech Republic'\n",
      " 'DK-Denmark' 'FI-Finland' 'FR-France' 'HU-Hungary' 'IS-Iceland'\n",
      " 'IN-India' 'IE-Ireland' 'IL-Israel' 'JP-Japan' 'KR-Korea (South)'\n",
      " 'LV-Latvia' 'LT-Lithuania' 'MX-Mexico' 'NL-Netherlands' 'NO-Norway'\n",
      " 'PH-Philippines' 'PL-Poland' 'RU-Russia' 'SK-Slovakia' 'SI-Slovenia'\n",
      " 'ZA-South Africa' 'ES-Spain' 'SE-Sweden' 'CH-Switzerland' 'TR-Turkey'\n",
      " 'US-United States' 'VE-Venezuela' 'BE-FLA-Belgium/ Flanders'\n",
      " 'BE-WAL-Belgium/ Wallonia' 'BE-BRU-Belgium/ Brussels' 'DE-W-Germany-West'\n",
      " 'DE-E-Germany-East'\n",
      " 'PT-Portugal 2012: first fieldwork round (main sample)'\n",
      " 'PT-Portugal 2012: second fieldwork round (complementary sample)'\n",
      " 'GB-GBN-Great Britain']\n",
      "==================== df_2022_clean ====================\n",
      "['40. AT-Austria' '36. AU-Australia' '100. BG-Bulgaria'\n",
      " '756. CH-Switzerland' '203. CZ-Czech Republic' '27601. DE-W-Germany-West'\n",
      " '27602. DE-E-Germany-East' '208. DK-Denmark' '724. ES-Spain'\n",
      " '246. FI-Finland' '250. FR-France' '300. GR-Greece' '191. HR-Croatia'\n",
      " '348. HU-Hungary' '37601. IL-J-Israel, Jews' '37602. IL-A-Israel, Arabs'\n",
      " '356. IN-India' '352. IS-Iceland' '380. IT-Italy' '392. JP-Japan'\n",
      " '440. LT-Lithuania' '528. NL-Netherlands' '578. NO-Norway'\n",
      " '554. NZ-New Zealand' '608. PH-Philippines' '616. PL-Poland'\n",
      " '643. RU-Russia' '752. SE-Sweden' '705. SI-Slovenia' '703. SK-Slovakia'\n",
      " '764. TH-Thailand' '158. TW-Taiwan' '840. US-United States'\n",
      " '710. ZA-South Africa']\n",
      "==================== CASEID ====================\n",
      "==================== df_2002_clean ====================\n",
      "[ 1000001  1000002  1000003 ... 39093341 39093342 39093344]\n",
      "==================== df_2012_clean ====================\n",
      "[2.01200032e+15 2.01200032e+15 2.01200032e+15 ... 2.01282601e+15\n",
      " 2.01282601e+15 2.01282601e+15]\n",
      "==================== df_2022_clean ====================\n",
      "[2.02200040e+15 2.02200040e+15 2.02200040e+15 ... 2.02271018e+15\n",
      " 2.02271018e+15 2.02271018e+15]\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_2002_clean, df_2012_clean, df_2022_clean]\n",
    "year = [2002, 2012, 2022]\n",
    "count = 0\n",
    "for col_name in df_mapping[\"COMMON_VAR\"]:\n",
    "    if col_name not in cleaned_vars:\n",
    "        count+=1\n",
    "        print(\"=\"*20,col_name, \"=\"*20)\n",
    "        for df, yr in zip(dfs, year):\n",
    "            print(\"=\"*20, f\"df_{yr}_clean\", \"=\"*20)\n",
    "            print(df[col_name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "d0b07d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "56cbd0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Defined cleaning functions for all common variables\n",
      "\n",
      "Function categories:\n",
      "  - Likert scales (5-point)\n",
      "  - Income deciles\n",
      "  - Work hours\n",
      "  - Work preferences\n",
      "  - Education levels\n",
      "  - Parent suitability\n",
      "  - Household counts\n",
      "  - Frequency scales\n",
      "  - Task fairness\n",
      "  - Task division\n",
      "  - Happiness scale\n",
      "  - Weekend decisions\n",
      "  - Cohabitation status\n",
      "  - Country codes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ==================== CLEANING FUNCTIONS FOR ALL COMMON VARIABLES ====================\n",
    "\n",
    "# 1. LIKERT SCALE (5-point: Strongly Agree to Strongly Disagree)\n",
    "def clean_likert_5(val):\n",
    "    \"\"\"Standardize 5-point Likert scale responses.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', 'nap', \"can't choose\", 'not available']):\n",
    "        return None\n",
    "    \n",
    "    # Map to standard categories\n",
    "    if 'strongly agree' in val_str or val_str.startswith('1.'):\n",
    "        return \"Strongly agree\"\n",
    "    if ('agree' in val_str and 'disagree' not in val_str and 'neither' not in val_str) or val_str.startswith('2.'):\n",
    "        return \"Agree\"\n",
    "    if 'neither' in val_str or val_str.startswith('3.'):\n",
    "        return \"Neither agree nor disagree\"\n",
    "    if ('disagree' in val_str and 'strongly' not in val_str) or val_str.startswith('4.'):\n",
    "        return \"Disagree\"\n",
    "    if 'strongly disagree' in val_str or val_str.startswith('5.'):\n",
    "        return \"Strongly disagree\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 2. INCOME DECILE (TOPBOT)\n",
    "def clean_income_decile(val):\n",
    "    \"\"\"Extract numeric decile from income scale (1-10).\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if any(x in val_str for x in [\"don't know\", 'no answer', 'refused', 'not available']):\n",
    "        return None\n",
    "    \n",
    "    # Handle text labels\n",
    "    if 'lowest' in val_str or 'bottom' in val_str or '01' in val_str or val_str.startswith('1.'):\n",
    "        return 1\n",
    "    if 'highest' in val_str or 'top' in val_str or '10' in val_str:\n",
    "        return 10\n",
    "    \n",
    "    # Extract numeric value\n",
    "    match = re.search(r'(\\d+)', val_str)\n",
    "    if match:\n",
    "        num = int(match.group(1))\n",
    "        if 1 <= num <= 10:\n",
    "            return num\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 3. WORK HOURS (SPWRKHRS, SP_HH_FAM, SP_HH, HH_FAM)\n",
    "def clean_work_hours(val):\n",
    "    \"\"\"Extract numeric work hours (0-95+).\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', 'nap', 'not available']):\n",
    "        return None\n",
    "    \n",
    "    # Handle \"none\"\n",
    "    if 'none' in val_str or 'no hour' in val_str:\n",
    "        return 0\n",
    "    \n",
    "    # Handle \"one hour\"\n",
    "    if 'one hour' in val_str or val_str == '1. 1 hour':\n",
    "        return 1\n",
    "    \n",
    "    # Handle 95+ \n",
    "    if '95' in val_str or '96' in val_str:\n",
    "        return 95\n",
    "    \n",
    "    # Extract numeric value (handle floats too)\n",
    "    match = re.search(r'(\\d+\\.?\\d*)', val_str)\n",
    "    if match:\n",
    "        num = float(match.group(1))\n",
    "        return int(num) if num <= 95 else 95\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 4. WORK PREFERENCE (WWYKS, WWYKUS)\n",
    "def clean_work_preference(val):\n",
    "    \"\"\"Standardize work preference categories.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", \"can't choose\"]):\n",
    "        return None\n",
    "    \n",
    "    # Women should decide\n",
    "    if 'women should decide' in val_str or 'women shld decide' in val_str or val_str.startswith('4.'):\n",
    "        return \"Women should decide\"\n",
    "    \n",
    "    # Work full-time\n",
    "    if 'full-time' in val_str or 'full time' in val_str or val_str.startswith('1.'):\n",
    "        return \"Work full-time\"\n",
    "    \n",
    "    # Work part-time\n",
    "    if 'part-time' in val_str or 'part time' in val_str or val_str.startswith('2.'):\n",
    "        return \"Work part-time\"\n",
    "    \n",
    "    # Stay at home\n",
    "    if 'stay at home' in val_str or val_str.startswith('3.'):\n",
    "        return \"Stay at home\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 5. EDUCATION LEVEL (SP_DEGREE)\n",
    "def clean_education(val):\n",
    "    \"\"\"Standardize education levels.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', 'nap', 'not available', 'not classifiable']):\n",
    "        return None\n",
    "    \n",
    "    # Map to standardized categories\n",
    "    if 'no formal' in val_str or 'no (formal) education' in val_str or val_str.startswith('0.'):\n",
    "        return \"No formal education\"\n",
    "    \n",
    "    if 'primary' in val_str and 'incomplete' not in val_str or val_str.startswith('1.'):\n",
    "        return \"Primary\"\n",
    "    \n",
    "    if 'lower secondary' in val_str or val_str.startswith('2.'):\n",
    "        return \"Lower secondary\"\n",
    "    \n",
    "    if 'upper secondary' in val_str or 'higher secondary' in val_str or val_str.startswith('3.'):\n",
    "        return \"Upper secondary\"\n",
    "    \n",
    "    if 'post secondary' in val_str or val_str.startswith('4.'):\n",
    "        return \"Post secondary\"\n",
    "    \n",
    "    if ('short-cycle tertiary' in val_str or 'lower level tertiary' in val_str or \n",
    "        'above lowest qualification' in val_str or val_str.startswith('5.') or val_str.startswith('6.')):\n",
    "        return \"Tertiary (Bachelor level)\"\n",
    "    \n",
    "    if ('upper level tertiary' in val_str or 'university degree' in val_str or \n",
    "        'above higher secondary' in val_str or val_str.startswith('7.')):\n",
    "        return \"Tertiary (Master level)\"\n",
    "    \n",
    "    if 'phd' in val_str or 'post tertiary' in val_str or val_str.startswith('8.'):\n",
    "        return \"Tertiary (Doctoral level)\"\n",
    "    \n",
    "    if 'lowest formal qualification' in val_str:\n",
    "        return \"Primary\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 6. PARENT SUITABILITY (MOMORFAF)\n",
    "def clean_parent_suit(val):\n",
    "    \"\"\"Standardize parent suitability scale.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", \"can't choose\"]):\n",
    "        return None\n",
    "    \n",
    "    # 2002 uses Likert, 2022 uses specific scale\n",
    "    if 'strongly agree' in val_str or 'mothers and fathers are equally' in val_str or val_str.startswith('3.'):\n",
    "        return \"Equally suited\"\n",
    "    \n",
    "    if ('agree' in val_str and 'strongly' not in val_str and 'neither' not in val_str and 'disagree' not in val_str) or \\\n",
    "       'mothers are somewhat better' in val_str or val_str.startswith('2.'):\n",
    "        return \"Mothers somewhat better\"\n",
    "    \n",
    "    if 'mothers are much better' in val_str or val_str.startswith('1.'):\n",
    "        return \"Mothers much better\"\n",
    "    \n",
    "    if 'fathers are somewhat better' in val_str or val_str.startswith('4.'):\n",
    "        return \"Fathers somewhat better\"\n",
    "    \n",
    "    if 'strongly disagree' in val_str or 'fathers are much better' in val_str or val_str.startswith('5.'):\n",
    "        return \"Fathers much better\"\n",
    "    \n",
    "    if 'neither' in val_str or 'disagree' in val_str:\n",
    "        return \"Equally suited\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 7. HOUSEHOLD COUNTS (HHTODD, HHCHILDR, HHADULT, HOMPOP)\n",
    "def clean_household_count(val):\n",
    "    \"\"\"Extract numeric count from household composition variables.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', 'refused', 'nap', 'not available']):\n",
    "        return None\n",
    "    \n",
    "    # Handle \"no children/adults/toddlers/persons\"\n",
    "    if any(x in val_str for x in ['no children', 'no toddlers', 'no adults', 'no persons']):\n",
    "        return 0\n",
    "    \n",
    "    # Handle \"one\"\n",
    "    if 'one child' in val_str or 'one toddler' in val_str or 'one adult' in val_str or 'one person' in val_str:\n",
    "        return 1\n",
    "    \n",
    "    # Extract numeric value\n",
    "    match = re.search(r'(\\d+)', val_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 8. FREQUENCY SCALE (FAM_DIF, DIFF_CONC_WORK, HH_TIRED, WORK_TIRED)\n",
    "FREQUENCY_ORDER = [\n",
    "    \"Several times a week\",\n",
    "    \"Several times a month\",\n",
    "    \"Once or twice\",\n",
    "    \"Never\",\n",
    "    \"NAP\"\n",
    "]\n",
    "\n",
    "def clean_frequency(val):\n",
    "    \"\"\"Standardize frequency scale.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', \"doesn't apply\", 'nap', 'not available', \"can't choose\"]):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    if 'several times a week' in val_str or val_str.startswith('1.'):\n",
    "        return \"Several times a week\"\n",
    "    \n",
    "    if 'several times a month' in val_str or val_str.startswith('2.'):\n",
    "        return \"Several times a month\"\n",
    "    \n",
    "    if 'once or twice' in val_str or val_str.startswith('3.'):\n",
    "        return \"Once or twice\"\n",
    "    \n",
    "    if 'never' in val_str or val_str.startswith('4.'):\n",
    "        return \"Never\"\n",
    "    \n",
    "    return \"NAP\"\n",
    "\n",
    "# 9. TASK FAIRNESS SHARE (SHARE_HH)\n",
    "FAIRNESS_SHARE_ORDER = [\n",
    "    \"Much more than fair share\",\n",
    "    \"Bit more than fair share\",\n",
    "    \"Fair share\",\n",
    "    \"Bit less than fair share\",\n",
    "    \"Much less than fair share\",\n",
    "    \"NAP\"\n",
    "]\n",
    "\n",
    "def clean_fairness_share(val):\n",
    "    \"\"\"Standardize task fairness perception.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', 'nap', \"can't choose\"]):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    if 'much more' in val_str or val_str.startswith('1.'):\n",
    "        return \"Much more than fair share\"\n",
    "    \n",
    "    if ('bit more' in val_str or 'a bit more' in val_str) or val_str.startswith('2.'):\n",
    "        return \"Bit more than fair share\"\n",
    "    \n",
    "    if 'roughly my fair share' in val_str or ('fair share' in val_str and 'more' not in val_str and 'less' not in val_str) or val_str.startswith('3.'):\n",
    "        return \"Fair share\"\n",
    "    \n",
    "    if ('bit less' in val_str or 'a bit less' in val_str) or val_str.startswith('4.'):\n",
    "        return \"Bit less than fair share\"\n",
    "    \n",
    "    if 'much less' in val_str or val_str.startswith('5.'):\n",
    "        return \"Much less than fair share\"\n",
    "    \n",
    "    return \"NAP\"\n",
    "\n",
    "# 10. TASK DIVISION (DIV_HH_* variables)\n",
    "TASK_DIV_ORDER = [\n",
    "    \"Always respondent\",\n",
    "    \"Usually respondent\",\n",
    "    \"About equal\",\n",
    "    \"Usually partner\",\n",
    "    \"Always partner\",\n",
    "    \"Third person\",\n",
    "    \"NAP\"\n",
    "]\n",
    "\n",
    "def clean_task_div(val):\n",
    "    \"\"\"Standardize task division categories.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', 'nap', \"can't choose\"]):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    # Third person first\n",
    "    if 'third person' in val_str or val_str.startswith('6.'):\n",
    "        return \"Third person\"\n",
    "    \n",
    "    # Always respondent/me\n",
    "    if ('always me' in val_str or 'always respondent' in val_str) or val_str.startswith('1.'):\n",
    "        return \"Always respondent\"\n",
    "    \n",
    "    # Usually respondent/me\n",
    "    if ('usually me' in val_str or 'usually respondent' in val_str) or val_str.startswith('2.'):\n",
    "        return \"Usually respondent\"\n",
    "    \n",
    "    # About equal\n",
    "    if 'about equal' in val_str or 'both together' in val_str or 'both equally' in val_str or val_str.startswith('3.'):\n",
    "        return \"About equal\"\n",
    "    \n",
    "    # Usually partner\n",
    "    if ('usually' in val_str and ('spouse' in val_str or 'partner' in val_str)) or val_str.startswith('4.'):\n",
    "        return \"Usually partner\"\n",
    "    \n",
    "    # Always partner\n",
    "    if ('always' in val_str and ('spouse' in val_str or 'partner' in val_str)) or val_str.startswith('5.'):\n",
    "        return \"Always partner\"\n",
    "    \n",
    "    return \"NAP\"\n",
    "\n",
    "# 11. HAPPINESS SCALE (LIFE_HAP)\n",
    "HAPPINESS_ORDER = [\n",
    "    \"Completely happy\",\n",
    "    \"Very happy\",\n",
    "    \"Fairly happy\",\n",
    "    \"Neither happy nor unhappy\",\n",
    "    \"Fairly unhappy\",\n",
    "    \"Very unhappy\",\n",
    "    \"Completely unhappy\"\n",
    "]\n",
    "\n",
    "def clean_happiness(val):\n",
    "    \"\"\"Standardize happiness scale.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", \"can't choose\"]):\n",
    "        return None\n",
    "    \n",
    "    if 'completely happy' in val_str or val_str.startswith('1.'):\n",
    "        return \"Completely happy\"\n",
    "    if 'very happy' in val_str or val_str.startswith('2.'):\n",
    "        return \"Very happy\"\n",
    "    if 'fairly happy' in val_str or val_str.startswith('3.'):\n",
    "        return \"Fairly happy\"\n",
    "    if 'neither' in val_str or val_str.startswith('4.'):\n",
    "        return \"Neither happy nor unhappy\"\n",
    "    if 'fairly unhappy' in val_str or val_str.startswith('5.'):\n",
    "        return \"Fairly unhappy\"\n",
    "    if 'very unhappy' in val_str or val_str.startswith('6.'):\n",
    "        return \"Very unhappy\"\n",
    "    if 'completely unhappy' in val_str or val_str.startswith('7.'):\n",
    "        return \"Completely unhappy\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 12. WEEKEND DECISION (HH_WEEKEND)\n",
    "def clean_weekend_decision(val):\n",
    "    \"\"\"Standardize weekend activity decision variable.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing/NAP\n",
    "    if any(x in val_str for x in ['no answer', \"don't know\", 'refused', 'nap', \"can't choose\"]):\n",
    "        return \"NAP\"\n",
    "    \n",
    "    # Map to task division categories for consistency\n",
    "    if 'always me' in val_str or 'mostly me' in val_str or val_str.startswith('1.'):\n",
    "        return \"Always respondent\"\n",
    "    \n",
    "    if 'usually me' in val_str or val_str.startswith('2.'):\n",
    "        return \"Usually respondent\"\n",
    "    \n",
    "    if 'we decide together' in val_str or 'about equal' in val_str or 'both together' in val_str or val_str.startswith('3.'):\n",
    "        return \"About equal\"\n",
    "    \n",
    "    if ('usually' in val_str and ('spouse' in val_str or 'partner' in val_str)) or val_str.startswith('4.'):\n",
    "        return \"Usually partner\"\n",
    "    \n",
    "    if ('always' in val_str and ('spouse' in val_str or 'partner' in val_str)) or 'mostly my spouse' in val_str or val_str.startswith('5.'):\n",
    "        return \"Always partner\"\n",
    "    \n",
    "    if 'third person' in val_str or 'someone else' in val_str or val_str.startswith('6.'):\n",
    "        return \"Third person\"\n",
    "    \n",
    "    if 'sometimes' in val_str:\n",
    "        return \"About equal\"\n",
    "    \n",
    "    return \"NAP\"\n",
    "\n",
    "# 13. COHABITATION (COHAB)\n",
    "def clean_cohab(val):\n",
    "    \"\"\"Standardize cohabitation status.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle missing\n",
    "    if any(x in val_str for x in ['no answer', 'refused', 'not available']):\n",
    "        return None\n",
    "    \n",
    "    if 'yes' in val_str and 'same household' in val_str or val_str.startswith('1.'):\n",
    "        return \"Partner, same household\"\n",
    "    \n",
    "    if 'yes' in val_str and \"don't live\" in val_str or val_str.startswith('2.'):\n",
    "        return \"Partner, different household\"\n",
    "    \n",
    "    if 'no partner' in val_str or val_str.startswith('3.'):\n",
    "        return \"No partner\"\n",
    "    \n",
    "    # Legacy coding\n",
    "    if val_str == 'yes':\n",
    "        return \"Partner, same household\"\n",
    "    if val_str == 'no':\n",
    "        return \"No partner\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 14. COUNTRY CODE (C_ALPHAN)\n",
    "def clean_country(val):\n",
    "    \"\"\"Extract standardized country code.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).upper()\n",
    "    \n",
    "    # Extract 2-letter ISO code at start or after dash\n",
    "    match = re.match(r'(\\d+\\.\\s*)?([A-Z]{2})', val_str)\n",
    "    if match:\n",
    "        return match.group(2)\n",
    "    \n",
    "    return val_str[:2] if len(val_str) >= 2 else None\n",
    "\n",
    "print(\"✓ Defined cleaning functions for all common variables\")\n",
    "print(\"\\nFunction categories:\")\n",
    "print(\"  - Likert scales (5-point)\")\n",
    "print(\"  - Income deciles\")\n",
    "print(\"  - Work hours\")\n",
    "print(\"  - Work preferences\")\n",
    "print(\"  - Education levels\")\n",
    "print(\"  - Parent suitability\")\n",
    "print(\"  - Household counts\")\n",
    "print(\"  - Frequency scales\")\n",
    "print(\"  - Task fairness\")\n",
    "print(\"  - Task division\")\n",
    "print(\"  - Happiness scale\")\n",
    "print(\"  - Weekend decisions\")\n",
    "print(\"  - Cohabitation status\")\n",
    "print(\"  - Country codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "d97f36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Applied cleaning to 34 variables across all years\n",
      "✓ Converted 12 variables to ordered categorical\n",
      "\n",
      "================================================================================\n",
      "SAMPLE OF CLEANED DATA\n",
      "================================================================================\n",
      "\n",
      "TOPBOT:\n",
      "  2002: {5.0: 6270, 6.0: 6125, 7.0: 4077}\n",
      "  2012: {5.0: 14677, 6.0: 10766, 7.0: 8080}\n",
      "  2022: {5.0: 10932, 6.0: 8891, 7.0: 7294}\n",
      "\n",
      "SPWRKHRS:\n",
      "  2002: {40.0: 5034, 50.0: 1285, 48.0: 1087}\n",
      "  2012: {40.0: 6698, 50.0: 1735, 45.0: 1450}\n",
      "  2022: {40.0: 5852, 50.0: 1358, 45.0: 1050}\n",
      "\n",
      "LIVWOMAR:\n",
      "  2002: {'Agree': 17856, 'Strongly agree': 11321, 'Disagree': 7154}\n",
      "  2012: {'Agree': 19334, 'Strongly agree': 12071, 'Disagree': 11740}\n",
      "  2022: {'Strongly agree': 15414, 'Agree': 14343, 'Neither agree nor disagree': 6579}\n",
      "\n",
      "WWYKS:\n",
      "  2002: {'Work part-time': 23770, 'Work full-time': 12451, 'Stay at home': 6377}\n",
      "  2012: {'Work part-time': 24981, 'Work full-time': 24783, 'Stay at home': 5960}\n",
      "  2022: {'Work full-time': 21289, 'Work part-time': 16511, 'Stay at home': 2766}\n",
      "\n",
      "SP_DEGREE:\n",
      "  2002: {'Upper secondary': 9193, 'Tertiary (Bachelor level)': 5139, 'Primary': 4372}\n",
      "  2012: {'Upper secondary': 10319, 'Lower secondary': 7419, 'Tertiary (Bachelor level)': 4809}\n",
      "  2022: {'Upper secondary': 9449, 'Tertiary (Bachelor level)': 6565, 'Tertiary (Master level)': 3321}\n",
      "\n",
      "FAM_DIF:\n",
      "  2002: {'NAP': 19342, 'Never': 10780, 'Once or twice': 8010}\n",
      "  2012: {'NAP': 25995, 'Never': 14269, 'Once or twice': 9442}\n",
      "  2022: {'NAP': 15837, 'Never': 10859, 'Once or twice': 8170}\n",
      "\n",
      "SHARE_HH:\n",
      "  2002: {'NAP': 17261, 'Fair share': 13163, 'Much more than fair share': 4983}\n",
      "  2012: {'NAP': 23547, 'Fair share': 15509, 'Much more than fair share': 7128}\n",
      "  2022: {'NAP': 18829, 'Fair share': 11795, 'Bit more than fair share': 5090}\n",
      "\n",
      "DIV_HH_COOK:\n",
      "  2002: {'NAP': 16528, 'Always respondent': 8380, 'About equal': 5981}\n",
      "  2012: {'NAP': 22717, 'Always respondent': 9935, 'About equal': 7564}\n",
      "  2022: {'NAP': 18496, 'About equal': 6598, 'Usually respondent': 6402}\n",
      "\n",
      "LIFE_HAP:\n",
      "  2002: {'Fairly happy': 18972, 'Very happy': 14301, 'Neither happy nor unhappy': 5930}\n",
      "  2012: {'Fairly happy': 24441, 'Very happy': 17829, 'Neither happy nor unhappy': 8373}\n",
      "  2022: {'Fairly happy': 18416, 'Very happy': 12618, 'Neither happy nor unhappy': 6615}\n",
      "\n",
      "COHAB:\n",
      "  2002: {'No partner': 15408, 'Partner, same household': 3806}\n",
      "  2012: {'Partner, same household': 38812, 'No partner': 18026}\n",
      "  2022: {'Partner, same household': 30403, 'No partner': 14813}\n"
     ]
    }
   ],
   "source": [
    "# ==================== APPLY CLEANING TO ALL VARIABLES ====================\n",
    "\n",
    "# Variable mapping: column_name -> cleaning_function\n",
    "cleaning_map = {\n",
    "    # Income decile\n",
    "    'TOPBOT': clean_income_decile,\n",
    "    \n",
    "    # Work hours\n",
    "    'SPWRKHRS': clean_work_hours,\n",
    "    'SP_HH_FAM': clean_work_hours,\n",
    "    'SP_HH': clean_work_hours,\n",
    "    'HH_FAM': clean_work_hours,\n",
    "    \n",
    "    # Work preferences\n",
    "    'WWYKS': clean_work_preference,\n",
    "    'WWYKUS': clean_work_preference,\n",
    "    \n",
    "    # Education\n",
    "    'SP_DEGREE': clean_education,\n",
    "    \n",
    "    # Parent suitability\n",
    "    'MOMORFAF': clean_parent_suit,\n",
    "    \n",
    "    # Household counts\n",
    "    'HHTODD': clean_household_count,\n",
    "    'HHCHILDR': clean_household_count,\n",
    "    'HHADULT': clean_household_count,\n",
    "    'HOMPOP': clean_household_count,\n",
    "    \n",
    "    # Likert scales\n",
    "    'LIVWOMAR': clean_likert_5,\n",
    "    'MEWH': clean_likert_5,\n",
    "    'HW_FULFIL': clean_likert_5,\n",
    "    'WO_WANT': clean_likert_5,\n",
    "    'WW_FAM_SUFFER': clean_likert_5,\n",
    "    'WW_CHILD_SUFFER': clean_likert_5,\n",
    "    'WW_WARM': clean_likert_5,\n",
    "    \n",
    "    # Frequency scales\n",
    "    'FAM_DIF': clean_frequency,\n",
    "    'DIFF_CONC_WORK': clean_frequency,\n",
    "    'HH_TIRED': clean_frequency,\n",
    "    'WORK_TIRED': clean_frequency,\n",
    "    \n",
    "    # Task fairness share\n",
    "    'SHARE_HH': clean_fairness_share,\n",
    "    \n",
    "    # Task division\n",
    "    'DIV_HH_COOK': clean_task_div,\n",
    "    'DIV_HH_CLEAN': clean_task_div,\n",
    "    'DIV_HH_GROC': clean_task_div,\n",
    "    'DIV_HH_CARE': clean_task_div,\n",
    "    'DIV_HH_LAUND': clean_task_div,\n",
    "    'HH_WEEKEND': clean_weekend_decision,\n",
    "    \n",
    "    # Happiness\n",
    "    'LIFE_HAP': clean_happiness,\n",
    "    \n",
    "    # Cohabitation\n",
    "    'COHAB': clean_cohab,\n",
    "    \n",
    "    # Country\n",
    "    'C_ALPHAN': clean_country,\n",
    "}\n",
    "\n",
    "# Apply cleaning functions to all dataframes\n",
    "for col_name, clean_func in cleaning_map.items():\n",
    "    for df in [df_2002_clean, df_2012_clean, df_2022_clean]:\n",
    "        if col_name in df.columns:\n",
    "            df[col_name] = df[col_name].apply(clean_func)\n",
    "\n",
    "print(f\"✓ Applied cleaning to {len(cleaning_map)} variables across all years\")\n",
    "\n",
    "# Convert categorical variables to ordered categories where appropriate\n",
    "categorical_mappings = {\n",
    "    'frequency': {\n",
    "        'columns': ['FAM_DIF', 'DIFF_CONC_WORK', 'HH_TIRED', 'WORK_TIRED'],\n",
    "        'order': FREQUENCY_ORDER\n",
    "    },\n",
    "    'task_division': {\n",
    "        'columns': ['DIV_HH_COOK', 'DIV_HH_CLEAN', 'DIV_HH_GROC', 'DIV_HH_CARE', 'DIV_HH_LAUND', 'HH_WEEKEND'],\n",
    "        'order': TASK_DIV_ORDER\n",
    "    },\n",
    "    'fairness_share': {\n",
    "        'columns': ['SHARE_HH'],\n",
    "        'order': FAIRNESS_SHARE_ORDER\n",
    "    },\n",
    "    'happiness': {\n",
    "        'columns': ['LIFE_HAP'],\n",
    "        'order': HAPPINESS_ORDER\n",
    "    }\n",
    "}\n",
    "\n",
    "for cat_type, info in categorical_mappings.items():\n",
    "    for col in info['columns']:\n",
    "        for df in [df_2002_clean, df_2012_clean, df_2022_clean]:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.Categorical(df[col], categories=info['order'], ordered=True)\n",
    "\n",
    "print(f\"✓ Converted {sum(len(v['columns']) for v in categorical_mappings.values())} variables to ordered categorical\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE OF CLEANED DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_vars = ['TOPBOT', 'SPWRKHRS', 'LIVWOMAR', 'WWYKS', 'SP_DEGREE', 'FAM_DIF', 'SHARE_HH', 'DIV_HH_COOK', 'LIFE_HAP', 'COHAB']\n",
    "for var in sample_vars:\n",
    "    if var in df_2022_clean.columns:\n",
    "        print(f\"\\n{var}:\")\n",
    "        print(f\"  2002: {df_2002_clean[var].value_counts().head(3).to_dict() if var in df_2002_clean.columns else 'N/A'}\")\n",
    "        print(f\"  2012: {df_2012_clean[var].value_counts().head(3).to_dict() if var in df_2012_clean.columns else 'N/A'}\")\n",
    "        print(f\"  2022: {df_2022_clean[var].value_counts().head(3).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89ccd5",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "\n",
    "All variables have been cleaned and standardized across years. Now saving to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "e80061f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved all cleaned dataframes to ../clean_csv/\n",
      "\n",
      "2002: (46638, 69)\n",
      "2012: (61754, 69)\n",
      "2022: (45762, 69)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataframes to CSV\n",
    "df_2002_clean.to_csv(\"../clean_csv/2002_clean.csv\", index=False)\n",
    "df_2012_clean.to_csv(\"../clean_csv/2012_clean.csv\", index=False)\n",
    "df_2022_clean.to_csv(\"../clean_csv/2022_clean.csv\", index=False)\n",
    "\n",
    "print(\"✓ Saved all cleaned dataframes to ../clean_csv/\")\n",
    "print(f\"\\n2002: {df_2002_clean.shape}\")\n",
    "print(f\"2012: {df_2012_clean.shape}\")\n",
    "print(f\"2022: {df_2022_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747c7d0",
   "metadata": {},
   "source": [
    "## Verify Data Consistency Across Years\n",
    "\n",
    "Check that cleaned variables have consistent values across all three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "5f51d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICATION OF DATA CONSISTENCY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TOPBOT\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0)]\n",
      "  Non-null count: 26626\n",
      "\n",
      "2012: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0)]\n",
      "  Non-null count: 56907\n",
      "\n",
      "2022: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0)]\n",
      "  Non-null count: 42193\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "SPWRKHRS\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: 88 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 17534\n",
      "\n",
      "2012: 93 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 22283\n",
      "\n",
      "2022: 84 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 17638\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "SP_HH_FAM\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: []\n",
      "  Non-null count: 0\n",
      "\n",
      "2012: 90 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 36016\n",
      "\n",
      "2022: 86 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 25256\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "SP_HH\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: 82 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 28212\n",
      "\n",
      "2012: 85 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 37069\n",
      "\n",
      "2022: 78 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 26091\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "HH_FAM\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: []\n",
      "  Non-null count: 0\n",
      "\n",
      "2012: 92 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 57019\n",
      "\n",
      "2022: 94 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 42009\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WWYKS\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Stay at home', 'Women should decide', 'Work full-time', 'Work part-time']\n",
      "  Non-null count: 43042\n",
      "\n",
      "2012: ['Stay at home', 'Women should decide', 'Work full-time', 'Work part-time']\n",
      "  Non-null count: 56248\n",
      "\n",
      "2022: ['Stay at home', 'Women should decide', 'Work full-time', 'Work part-time']\n",
      "  Non-null count: 40630\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WWYKUS\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Stay at home', 'Women should decide', 'Work full-time', 'Work part-time']\n",
      "  Non-null count: 43065\n",
      "\n",
      "2012: ['Stay at home', 'Women should decide', 'Work full-time', 'Work part-time']\n",
      "  Non-null count: 56134\n",
      "\n",
      "2022: ['Stay at home', 'Women should decide', 'Work full-time', 'Work part-time']\n",
      "  Non-null count: 40414\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "SP_DEGREE\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['No formal education', 'Primary', 'Tertiary (Bachelor level)', 'Tertiary (Master level)', 'Upper secondary']\n",
      "  Non-null count: 24169\n",
      "\n",
      "2012: ['Lower secondary', 'No formal education', 'Primary', 'Tertiary (Bachelor level)', 'Tertiary (Master level)', 'Upper secondary']\n",
      "  Non-null count: 31572\n",
      "\n",
      "2022: ['Lower secondary', 'No formal education', 'Post secondary', 'Primary', 'Tertiary (Bachelor level)', 'Tertiary (Doctoral level)', 'Tertiary (Master level)', 'Upper secondary']\n",
      "  Non-null count: 26060\n",
      "\n",
      "Status: ⚠ CHECK NEEDED\n",
      "\n",
      "================================================================================\n",
      "MOMORFAF\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Equally suited', 'Fathers much better', 'Mothers somewhat better']\n",
      "  Non-null count: 43605\n",
      "\n",
      "2012: []\n",
      "  Non-null count: 0\n",
      "\n",
      "2022: ['Equally suited', 'Fathers much better', 'Fathers somewhat better', 'Mothers much better', 'Mothers somewhat better']\n",
      "  Non-null count: 44744\n",
      "\n",
      "Status: ⚠ CHECK NEEDED\n",
      "\n",
      "================================================================================\n",
      "HHTODD\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0)]\n",
      "  Non-null count: 7207\n",
      "\n",
      "2012: 11 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 58959\n",
      "\n",
      "2022: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0)]\n",
      "  Non-null count: 41701\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "HHCHILDR\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0)]\n",
      "  Non-null count: 13917\n",
      "\n",
      "2012: 17 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 58988\n",
      "\n",
      "2022: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0)]\n",
      "  Non-null count: 43473\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "HHADULT\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: 15 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 42583\n",
      "\n",
      "2012: []\n",
      "  Non-null count: 0\n",
      "\n",
      "2022: 17 unique values\n",
      "  Sample: [np.float64(0.0), np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0)]\n",
      "  Non-null count: 44242\n",
      "\n",
      "Status: ⚠ CHECK NEEDED\n",
      "\n",
      "================================================================================\n",
      "HOMPOP\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: 23 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 46202\n",
      "\n",
      "2012: 30 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 58990\n",
      "\n",
      "2022: 21 unique values\n",
      "  Sample: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "  Non-null count: 44836\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "LIVWOMAR\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45270\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 57817\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 44905\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "MEWH\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45475\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 58078\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45045\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "HW_FULFIL\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 43720\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 55612\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 43029\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WO_WANT\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 44089\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 56379\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 43742\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WW_FAM_SUFFER\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45120\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 57631\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 44784\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WW_CHILD_SUFFER\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45028\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 57551\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 44626\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WW_WARM\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45484\n",
      "\n",
      "2012: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 57860\n",
      "\n",
      "2022: ['Agree', 'Disagree', 'Neither agree nor disagree', 'Strongly agree', 'Strongly disagree']\n",
      "  Non-null count: 45008\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "FAM_DIF\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "DIFF_CONC_WORK\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "HH_TIRED\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "WORK_TIRED\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Several times a week', 'Several times a month', 'Once or twice', 'Never', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "SHARE_HH\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Much more than fair share', 'Bit more than fair share', 'Fair share', 'Bit less than fair share', 'Much less than fair share', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Much more than fair share', 'Bit more than fair share', 'Fair share', 'Bit less than fair share', 'Much less than fair share', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Much more than fair share', 'Bit more than fair share', 'Fair share', 'Bit less than fair share', 'Much less than fair share', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "DIV_HH_COOK\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "DIV_HH_CLEAN\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "DIV_HH_GROC\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "DIV_HH_CARE\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "DIV_HH_LAUND\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "HH_WEEKEND\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022 (categorical): ['Always respondent', 'Usually respondent', 'About equal', 'Usually partner', 'Always partner', 'Third person', 'NAP']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "LIFE_HAP\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002 (categorical): ['Completely happy', 'Very happy', 'Fairly happy', 'Neither happy nor unhappy', 'Fairly unhappy', 'Very unhappy', 'Completely unhappy']\n",
      "  Non-null count: 45800\n",
      "\n",
      "2012 (categorical): ['Completely happy', 'Very happy', 'Fairly happy', 'Neither happy nor unhappy', 'Fairly unhappy', 'Very unhappy', 'Completely unhappy']\n",
      "  Non-null count: 60844\n",
      "\n",
      "2022 (categorical): ['Completely happy', 'Very happy', 'Fairly happy', 'Neither happy nor unhappy', 'Fairly unhappy', 'Very unhappy', 'Completely unhappy']\n",
      "  Non-null count: 45018\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "COHAB\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: ['No partner', 'Partner, same household']\n",
      "  Non-null count: 19214\n",
      "\n",
      "2012: ['No partner', 'Partner, same household']\n",
      "  Non-null count: 56838\n",
      "\n",
      "2022: ['No partner', 'Partner, same household']\n",
      "  Non-null count: 45216\n",
      "\n",
      "Status: ✓ CONSISTENT\n",
      "\n",
      "================================================================================\n",
      "C_ALPHAN\n",
      "================================================================================\n",
      "Available in years: [2002, 2012, 2022]\n",
      "\n",
      "2002: 33 unique values\n",
      "  Sample: ['AT', 'AU', 'BE', 'BG', 'BR']\n",
      "  Non-null count: 46638\n",
      "\n",
      "2012: 41 unique values\n",
      "  Sample: ['AR', 'AT', 'AU', 'BE', 'BG']\n",
      "  Non-null count: 61754\n",
      "\n",
      "2022: 32 unique values\n",
      "  Sample: ['AT', 'AU', 'BG', 'CH', 'CZ']\n",
      "  Non-null count: 45762\n",
      "\n",
      "Status: ⚠ CHECK NEEDED\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Total variables checked: 34\n",
      "Consistent: 30\n",
      "Need review: 4\n"
     ]
    }
   ],
   "source": [
    "# Verification: Check consistency of cleaned variables across years\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION OF DATA CONSISTENCY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Variables to verify (sample from each type)\n",
    "verify_vars = [\n",
    "    'TOPBOT', 'SPWRKHRS', 'SP_HH_FAM', 'SP_HH', 'HH_FAM',\n",
    "    'WWYKS', 'WWYKUS', 'SP_DEGREE', 'MOMORFAF',\n",
    "    'HHTODD', 'HHCHILDR', 'HHADULT', 'HOMPOP',\n",
    "    'LIVWOMAR', 'MEWH', 'HW_FULFIL', 'WO_WANT', \n",
    "    'WW_FAM_SUFFER', 'WW_CHILD_SUFFER', 'WW_WARM',\n",
    "    'FAM_DIF', 'DIFF_CONC_WORK', 'HH_TIRED', 'WORK_TIRED',\n",
    "    'SHARE_HH', 'DIV_HH_COOK', 'DIV_HH_CLEAN', 'DIV_HH_GROC',\n",
    "    'DIV_HH_CARE', 'DIV_HH_LAUND', 'HH_WEEKEND',\n",
    "    'LIFE_HAP', 'COHAB', 'C_ALPHAN'\n",
    "]\n",
    "\n",
    "dfs_dict = {\n",
    "    2002: df_2002_clean,\n",
    "    2012: df_2012_clean,\n",
    "    2022: df_2022_clean\n",
    "}\n",
    "\n",
    "consistency_report = []\n",
    "\n",
    "for var in verify_vars:\n",
    "    # Check if variable exists in at least one df\n",
    "    exists_in = [year for year, df in dfs_dict.items() if var in df.columns]\n",
    "    \n",
    "    if not exists_in:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{var}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Available in years: {exists_in}\")\n",
    "    \n",
    "    # Get unique values from each year\n",
    "    for year in [2002, 2012, 2022]:\n",
    "        df = dfs_dict[year]\n",
    "        if var in df.columns:\n",
    "            unique_vals = df[var].dropna().unique()\n",
    "            \n",
    "            # For categorical, show categories\n",
    "            if hasattr(df[var], 'cat'):\n",
    "                print(f\"\\n{year} (categorical): {df[var].cat.categories.tolist()}\")\n",
    "                print(f\"  Non-null count: {df[var].notna().sum()}\")\n",
    "            else:\n",
    "                # Show sample of unique values\n",
    "                if len(unique_vals) <= 10:\n",
    "                    print(f\"\\n{year}: {sorted(unique_vals)}\")\n",
    "                else:\n",
    "                    print(f\"\\n{year}: {len(unique_vals)} unique values\")\n",
    "                    print(f\"  Sample: {sorted(unique_vals)[:5]}\")\n",
    "                print(f\"  Non-null count: {df[var].notna().sum()}\")\n",
    "        else:\n",
    "            print(f\"\\n{year}: Not available\")\n",
    "    \n",
    "    # Check consistency\n",
    "    all_values = []\n",
    "    for year, df in dfs_dict.items():\n",
    "        if var in df.columns:\n",
    "            if hasattr(df[var], 'cat'):\n",
    "                all_values.extend(df[var].cat.categories.tolist())\n",
    "            else:\n",
    "                all_values.extend(df[var].dropna().unique().tolist())\n",
    "    \n",
    "    unique_across_years = len(set(str(v) for v in all_values))\n",
    "    \n",
    "    # Determine if consistent\n",
    "    is_consistent = True\n",
    "    if len(exists_in) > 1:\n",
    "        # Compare value types across years\n",
    "        value_sets = []\n",
    "        for year, df in dfs_dict.items():\n",
    "            if var in df.columns:\n",
    "                if hasattr(df[var], 'cat'):\n",
    "                    value_sets.append(set(df[var].cat.categories.tolist()))\n",
    "                else:\n",
    "                    vals = df[var].dropna().unique()\n",
    "                    value_sets.append(set(str(v) for v in vals))\n",
    "        \n",
    "        if len(value_sets) > 1:\n",
    "            # Check if there's significant overlap or if they're all numeric\n",
    "            if all(df[var].dtype in ['int64', 'float64'] for year, df in dfs_dict.items() if var in df.columns):\n",
    "                is_consistent = True  # Numeric variables are consistent\n",
    "            elif len(value_sets[0]) > 0:\n",
    "                # For categorical, check if categories are similar\n",
    "                all_cats = set()\n",
    "                for vs in value_sets:\n",
    "                    all_cats.update(vs)\n",
    "                # If all years share same categories, consistent\n",
    "                is_consistent = all(vs == value_sets[0] for vs in value_sets)\n",
    "    \n",
    "    status = \"✓ CONSISTENT\" if is_consistent else \"⚠ CHECK NEEDED\"\n",
    "    print(f\"\\nStatus: {status}\")\n",
    "    consistency_report.append((var, status, exists_in))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "consistent_count = sum(1 for _, status, _ in consistency_report if \"CONSISTENT\" in status)\n",
    "print(f\"Total variables checked: {len(consistency_report)}\")\n",
    "print(f\"Consistent: {consistent_count}\")\n",
    "print(f\"Need review: {len(consistency_report) - consistent_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cdf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_vars = [\n",
    "    \"urban_rural\",\n",
    "    \"spouse_work_status\",\n",
    "    \"sex\",\n",
    "    \"code_higher_income\",\n",
    "    \"code_income_control\",\n",
    "    \"hh_wrk_hrs\",\n",
    "    \"work_status\",\n",
    "    \"marital\",\n",
    "    \"wrk_hrs\",\n",
    "    \"educ_4_label\",\n",
    "    \"age\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e8d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002_clean = pd.read_csv(\"../clean_csv/2002_clean.csv\")\n",
    "df_2012_clean = pd.read_csv(\"../clean_csv/2012_clean.csv\")\n",
    "df_2022_clean = pd.read_csv(\"../clean_csv/2022_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18974d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002_clean[\"marital_new\"] = df_2002[\"v202\"]\n",
    "df_2012_clean[\"marital_new\"] = df_2012[\"MARITAL\"]\n",
    "df_2022_clean[\"marital_new\"] = df_2022[\"MARITAL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5947beed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002 Marital Status Distribution:\n",
      "marital_new\n",
      "Married      27103\n",
      "Single       11396\n",
      "Widowed       3781\n",
      "Divorced      2944\n",
      "Separated     1033\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2012 Marital Status Distribution:\n",
      "marital_new\n",
      "Married              34308\n",
      "Single               15081\n",
      "Widowed               5158\n",
      "Divorced              4197\n",
      "Civil partnership     1265\n",
      "Separated             1132\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2022 Marital Status Distribution:\n",
      "marital_new\n",
      "Married              22862\n",
      "Single               12857\n",
      "Divorced              3726\n",
      "Widowed               3334\n",
      "Civil partnership     1221\n",
      "Separated              887\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def clean_marital_status(val, year):\n",
    "    \"\"\"\n",
    "    Standardizes marital status categories across years.\n",
    "    Returns: Married, Civil partnership, Widowed, Divorced, Separated, Single, or None\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    \n",
    "    val_str = str(val).lower()\n",
    "    \n",
    "    # Handle refused/no answer\n",
    "    if 'refused' in val_str or 'no answer' in val_str or val_str.startswith('-'):\n",
    "        return None\n",
    "    \n",
    "    # Civil partnership (separate from marriage)\n",
    "    if 'civil partnership' in val_str and 'never' not in val_str and \"separate\" not in val_str:\n",
    "        return \"Civil partnership\"\n",
    "    \n",
    "    # Married (but NOT civil partnership)\n",
    "    if ('married' in val_str or 'marr' in val_str) and 'civil partnership' not in val_str:\n",
    "        if 'never' not in val_str and 'separated' not in val_str and 'divorced' not in val_str:\n",
    "            return \"Married\"\n",
    "    \n",
    "    # Widowed\n",
    "    if 'widow' in val_str or 'died' in val_str:\n",
    "        return \"Widowed\"\n",
    "    \n",
    "    # Divorced\n",
    "    if 'divorced' in val_str or 'legally separated' in val_str:\n",
    "        return \"Divorced\"\n",
    "    \n",
    "    # Separated (but still legally married/in partnership)\n",
    "    if 'separated' in val_str and 'divorced' not in val_str and 'legally separated' not in val_str:\n",
    "        return \"Separated\"\n",
    "    \n",
    "    # Single\n",
    "    if 'single' in val_str or 'never' in val_str:\n",
    "        return \"Single\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Apply to all three dataframes\n",
    "df_2002_clean[\"marital_new\"] = df_2002_clean[\"marital_new\"].apply(lambda x: clean_marital_status(x, 2002))\n",
    "df_2012_clean[\"marital_new\"] = df_2012_clean[\"marital_new\"].apply(lambda x: clean_marital_status(x, 2012))\n",
    "df_2022_clean[\"marital_new\"] = df_2022_clean[\"marital_new\"].apply(lambda x: clean_marital_status(x, 2022))\n",
    "\n",
    "# Check the distribution\n",
    "print(\"2002 Marital Status Distribution:\")\n",
    "print(df_2002_clean[\"marital_new\"].value_counts())\n",
    "print(\"\\n2012 Marital Status Distribution:\")\n",
    "print(df_2012_clean[\"marital_new\"].value_counts())\n",
    "print(\"\\n2022 Marital Status Distribution:\")\n",
    "print(df_2022_clean[\"marital_new\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889472ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002_clean[\"marital\"] = df_2002_clean[\"marital_new\"]\n",
    "df_2012_clean[\"marital\"] = df_2012_clean[\"marital_new\"]\n",
    "df_2022_clean[\"marital\"] = df_2022_clean[\"marital_new\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50e35a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002_clean.drop(\"marital_new\",axis=1, inplace=True)\n",
    "df_2012_clean.drop(\"marital_new\",axis=1, inplace=True)\n",
    "df_2022_clean.drop(\"marital_new\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad48b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marital']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col for col in df_2022_clean.columns if col.startswith(\"marital\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a87787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved all cleaned dataframes to ../clean_csv/\n",
      "\n",
      "2002: (46638, 69)\n",
      "2012: (61754, 69)\n",
      "2022: (45762, 69)\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataframes to CSV\n",
    "df_2002_clean.to_csv(\"../clean_csv/2002_clean.csv\", index=False)\n",
    "df_2012_clean.to_csv(\"../clean_csv/2012_clean.csv\", index=False)\n",
    "df_2022_clean.to_csv(\"../clean_csv/2022_clean.csv\", index=False)\n",
    "\n",
    "print(\"✓ Saved all cleaned dataframes to ../clean_csv/\")\n",
    "print(f\"\\n2002: {df_2002_clean.shape}\")\n",
    "print(f\"2012: {df_2012_clean.shape}\")\n",
    "print(f\"2022: {df_2022_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53c6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
